{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet trained with baseline data + TorchIO augmented data\n",
    "\n",
    "## Qimin Zhang and Weiwei Qi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *0427 Revisit: use binary mask*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Link: https://arxiv.org/abs/2003.04696\n",
      "\n",
      "TorchIO version: 0.15.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import enum\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "from unet import U_Net\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchio\n",
    "from torchio import AFFINE, DATA, PATH, TYPE, STEM\n",
    "\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy import stats\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('TorchIO version:', torchio.__version__)\n",
    "\n",
    "training_split_ratio = 0.9\n",
    "num_epochs = 5\n",
    "compute_histograms = False\n",
    "train_whole_images = False\n",
    "train_patches = False\n",
    "\n",
    "from torchio.transforms import (\n",
    "    RandomFlip, # check axis\n",
    "    RandomAffine,\n",
    "    RandomElasticDeformation,\n",
    "    RandomNoise, # lower noise\n",
    "    RandomMotion, # no use\n",
    "    RandomBiasField,\n",
    "    RandomSpike,\n",
    "    RandomBlur,\n",
    "    Rescale,\n",
    "    Resample,\n",
    "    ToCanonical,\n",
    "    ZNormalization,\n",
    "    CenterCropOrPad,\n",
    "    HistogramStandardization,\n",
    "    Interpolation,\n",
    "    RandomGhosting\n",
    ")\n",
    "\n",
    "seed = 4460\n",
    "torch.manual_seed(4460)\n",
    "np.random.seed(4460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation Details\n",
      "\tDevice Used: (cuda)  Tesla K80\n",
      "\n",
      "Packages Used Versions:-\n",
      "\tPytorch Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Computation Details')\n",
    "print(f'\\tDevice Used: ({device})  {torch.cuda.get_device_name(torch.cuda.current_device())}\\n')\n",
    "\n",
    "print('Packages Used Versions:-')\n",
    "print(f'\\tPytorch Version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a U-Net with original data + TorchIO augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folder used\n",
    "DATASET_PATH = os.path.join('./data')\n",
    "\n",
    "# We would like to perform a train-validation-test split at the ratio of T:V:T = 8:1:1.\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Batch size for training. Limited by GPU memory\n",
    "BATCH_SIZE = 6\n",
    "# Training Epochs\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/all_gbm_pre_reg/BraTS19_2013_0_1_t1reg.nii.gz\n",
      "./data/all_tumors_reg/BraTS19_2013_0_1_seg_reg.nii.gz\n"
     ]
    }
   ],
   "source": [
    "print(sorted(glob(\"./data/all_gbm_pre_reg/*_t1reg.nii.gz\"))[0])\n",
    "print(sorted(glob(\"./data/all_tumors_reg/*_seg_reg.nii.gz\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(Dataset):\n",
    "    '''\n",
    "    Returns a TumorDataset class object which represents our tumor dataset.\n",
    "    TumorDataset inherits from torch.utils.data.Dataset class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, DEBUG = False):\n",
    "        '''\n",
    "        Constructor for our TumorDataset class.\n",
    "        Parameters:\n",
    "            root_dir(str): Directory with all the images.\n",
    "            DEBUG(bool): To switch to debug mode for image transformation.\n",
    "\n",
    "        Returns: None\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        # The default transformation is composed of \n",
    "        # 1) a grayscale conversion.\n",
    "        self.default_transformation = transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        self.DEBUG = DEBUG\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Overridden method from inheritted class to support\n",
    "        indexing of dataset such that datset[I] can be used\n",
    "        to get Ith sample.\n",
    "        Parameters:\n",
    "            index(int): Index of the dataset sample\n",
    "            \n",
    "        Return:\n",
    "            sample(dict): Contains the index, image, mask torch.Tensor.\n",
    "                        'index': Index of the image.\n",
    "                        'image': Contains the tumor image torch.Tensor.\n",
    "                        'mask' : Contains the mask image torch.Tensor.\n",
    "        '''\n",
    "        # Find the filenames for the tumor images and masks.\n",
    "        image_path = os.path.join(self.root_dir, \"all_gbm_pre_reg\")\n",
    "        tumor_path = os.path.join(self.root_dir, \"all_tumors_reg\")\n",
    "        \n",
    "        image_name = sorted(glob(os.path.join(image_path, \"*t1reg.nii.gz\")))[index]\n",
    "        mask_name = sorted(glob(os.path.join(tumor_path, \"*seg_reg.nii.gz\")))[index]\n",
    "\n",
    "        # Use nibabel to open the images and masks.\n",
    "        image = nib.load(image_name).get_fdata()\n",
    "        mask = nib.load(mask_name).get_fdata()\n",
    "        mask = (mask > 0)\n",
    "\n",
    "        # Apply the default transformations on the images and masks.\n",
    "        #image = self.default_transformation(image)\n",
    "        #mask = self.default_transformation(mask)\n",
    "\n",
    "        # Convert the images and masks to tensor.\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Construct the images and masks together in the form of a dictionary.\n",
    "        sample = {'index': index, 'image': image, 'mask': mask}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Overridden method from inheritted class so that\n",
    "        len(self) returns the size of the dataset.\n",
    "        '''\n",
    "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
    "        total_image_files = len(glob(os.path.join(self.root_dir, 'all_gbm_pre_reg', '*t1reg.nii.gz')))\n",
    "        total_tumor_files = len(glob(os.path.join(self.root_dir, 'all_tumors_reg', '*seg_reg.nii.gz')))\n",
    "\n",
    "        # Sanity check: the number of files shall be even since tumor images and masks are in pairs.\n",
    "        assert total_image_files == total_tumor_files, error_msg\n",
    "        \n",
    "        # Return how many image-mask pairs we have.\n",
    "        return total_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset_Aug(Dataset):\n",
    "    '''\n",
    "    Returns a TumorDataset class object which represents our tumor dataset.\n",
    "    TumorDataset inherits from torch.utils.data.Dataset class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, DEBUG = False):\n",
    "        '''\n",
    "        Constructor for our TumorDataset class.\n",
    "        Parameters:\n",
    "            root_dir(str): Directory with all the images.\n",
    "            DEBUG(bool): To switch to debug mode for image transformation.\n",
    "\n",
    "        Returns: None\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        # The default transformation is composed of \n",
    "        # 1) a grayscale conversion.\n",
    "        self.default_transformation = transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        self.DEBUG = DEBUG\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Overridden method from inheritted class to support\n",
    "        indexing of dataset such that datset[I] can be used\n",
    "        to get Ith sample.\n",
    "        Parameters:\n",
    "            index(int): Index of the dataset sample\n",
    "            \n",
    "        Return:\n",
    "            sample(dict): Contains the index, image, mask torch.Tensor.\n",
    "                        'index': Index of the image.\n",
    "                        'image': Contains the tumor image torch.Tensor.\n",
    "                        'mask' : Contains the mask image torch.Tensor.\n",
    "        '''\n",
    "        # Find the filenames for the tumor images and masks.\n",
    "        image_path = os.path.join(self.root_dir, \"all_gbm_pre_reg\")\n",
    "        tumor_path = os.path.join(self.root_dir, \"all_tumors_reg\")\n",
    "        \n",
    "        image_name = sorted(glob(os.path.join(image_path, \"*t1reg.nii.gz\")))[index]\n",
    "        mask_name = sorted(glob(os.path.join(tumor_path, \"*seg_reg.nii.gz\")))[index]\n",
    "\n",
    "        # Use nibabel to open the images and masks.\n",
    "        image = nib.load(image_name)\n",
    "        mask = nib.load(mask_name)\n",
    "\n",
    "        # Apply the default transformations on the images and masks.\n",
    "        #image = self.default_transformation(image)\n",
    "        #mask = self.default_transformation(mask)\n",
    "\n",
    "        # Convert the images and masks to tensor.\n",
    "        #image = TF.to_tensor(image)\n",
    "        #mask = TF.to_tensor(mask)\n",
    "        \n",
    "#        transfrom_randint = np.random.choice(2,1,p=[0.5,0.5]).item()\n",
    "        \n",
    "#         if transfrom_randint == 0:\n",
    "            \n",
    "#             image = image.get_fdata()\n",
    "#             mask = mask.get_fdata()\n",
    "            \n",
    "#             image = torch.tensor(image).type('torch.DoubleTensor')\n",
    "#             mask = torch.tensor(mask).type('torch.DoubleTensor')\n",
    "            \n",
    "#             sample = {'index': index, 'image': image, 'mask': mask}\n",
    "        \n",
    "#        elif transfrom_randint == 1:\n",
    "            \n",
    "        # no transform, RandomAffine, RandomElasticDeformation, RandomFlip\n",
    "        spatial_augmentation_type = np.random.choice(4,1, p = [0.1, 0.4, 0.25, 0.25]).item()\n",
    "        # no transform, RandomNoise, RandomBiasField, RandomMotion, RandomGhosting\n",
    "        # For now only introduce random Gaussian noise\n",
    "        intensity_augmentation_type = np.random.choice(4,1, p = [0.05, 0.48, 0.36, 0.11]).item()\n",
    "\n",
    "        if spatial_augmentation_type == 1:\n",
    "            spatial_transform = RandomAffine(scales=(0.9,1.1),degrees=(-6,6),\n",
    "                                            image_interpolation=Interpolation.NEAREST,seed=seed)\n",
    "        elif spatial_augmentation_type == 2:\n",
    "            spatial_transform = RandomElasticDeformation(num_control_points=20,locked_borders=2,\n",
    "                                                        proportion_to_augment=1, \n",
    "                                                        image_interpolation=Interpolation.NEAREST,\n",
    "                                                        seed=seed) \n",
    "                                                         #deformation_std = 50)\n",
    "        elif spatial_augmentation_type == 3:\n",
    "            axis = np.random.choice([0,2],1).item()\n",
    "            spatial_transform = RandomFlip(seed = seed, flip_probability = 1, axes = axis)\n",
    "\n",
    "        if intensity_augmentation_type == 1: \n",
    "            intensity_transform = RandomNoise(seed = seed, std = (0, 5))\n",
    "        elif intensity_augmentation_type == 2: \n",
    "            intensity_transform = RandomBlur(std=(0, 0.5),seed=seed)\n",
    "        elif intensity_augmentation_type == 3:\n",
    "            intensity_transform = RandomGhosting(seed = seed, \n",
    "                                                 proportion_to_augment = 1, \n",
    "                                                 num_ghosts = (1, 3))\n",
    "\n",
    "        subject = {\n",
    "            'image': {DATA: torch.from_numpy(image.get_fdata().astype(np.float32)).unsqueeze(0), AFFINE: image.affine, TYPE: torchio.INTENSITY},\n",
    "            'mask': {DATA: torch.from_numpy((mask.get_fdata() > 0).astype(np.float32)).unsqueeze(0), AFFINE: mask.affine, TYPE: torchio.LABEL}\n",
    "            }\n",
    "\n",
    "        # Apply spatial transform. \n",
    "        # spatial_transformed_subject = spatial_transform(subject) if spatial_augmentation_type > 0 else subject\n",
    "        transformed_subject = spatial_transform(subject) if spatial_augmentation_type > 0 else subject\n",
    "        transformed_subject = intensity_transform(transformed_subject) if intensity_augmentation_type > 0 else transformed_subject\n",
    "\n",
    "        image = np.squeeze(transformed_subject['image']['data'].numpy())\n",
    "        mask = np.squeeze(transformed_subject['mask']['data'].numpy())\n",
    "\n",
    "        image = torch.tensor(image).type('torch.DoubleTensor').permute(2, 0, 1)\n",
    "        mask = torch.tensor(mask).type('torch.DoubleTensor').permute(2, 0, 1)\n",
    "\n",
    "        sample = {'index': index, 'image': image, 'mask': mask}\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Overridden method from inheritted class so that\n",
    "        len(self) returns the size of the dataset.\n",
    "        '''\n",
    "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
    "        total_image_files = len(glob(os.path.join(self.root_dir, 'all_gbm_pre_reg', '*t1reg.nii.gz')))\n",
    "        total_tumor_files = len(glob(os.path.join(self.root_dir, 'all_tumors_reg', '*seg_reg.nii.gz')))\n",
    "\n",
    "        # Sanity check: the number of files shall be even since tumor images and masks are in pairs.\n",
    "        assert total_image_files == total_tumor_files, error_msg\n",
    "        \n",
    "        # Return how many image-mask pairs we have.\n",
    "        return total_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(length, val_split, test_split):\n",
    "    '''\n",
    "    Gets the Training & Testing data indices for the dataset.\n",
    "    Stores the indices and returns them back when the same dataset is used.\n",
    "    Inputs:\n",
    "        length(int): Length of the dataset used.\n",
    "        val_split: the portion (0 to 1) of data used for validation.\n",
    "        test_split: the portion (0 to 1) of data used for testing.\n",
    "    Parameters:\n",
    "        train_indices(list): Array of indices used for training purpose.\n",
    "        validation_indices(list): Array of indices used for validation purpose.\n",
    "        test_indices(list): Array of indices used for testing purpose.\n",
    "    '''\n",
    "    data = dict()\n",
    "    indices = list(range(length))\n",
    "    np.random.shuffle(indices)\n",
    "    split1 = int(np.floor(test_split * len(tumor_dataset)))\n",
    "    split2 = split1 + int(np.floor(val_split * len(tumor_dataset)))\n",
    "    train_indices, validation_indices, test_indices = indices[split2:], indices[split1:split2], indices[:split1]\n",
    "    return train_indices, validation_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(predicted, target):\n",
    "    '''\n",
    "    Calculates the Sørensen–Dice Coefficient for a single sample.\n",
    "    Parameters:\n",
    "        predicted(numpy.ndarray): Predicted single output of the network.\n",
    "                                Shape - (Channel, Height, Width)\n",
    "        target(numpy.ndarray): Actual required single output for the network\n",
    "                                Shape - (Channel, Height, Width)\n",
    "\n",
    "    Returns:\n",
    "        coefficient(float): Dice coefficient for the input sample.\n",
    "                                    1 represents highest similarity and\n",
    "                                    0 represents lowest similarity.\n",
    "    '''\n",
    "    # The smooth term is used to prevent division by zero.\n",
    "    smooth = 1\n",
    "    product = np.multiply(predicted, target)\n",
    "    intersection = np.sum(product)\n",
    "    coefficient = (2 * intersection + smooth) / (np.sum(predicted) + np.sum(target) + smooth)\n",
    "    return coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the train set: 269 \n",
      "Number of files in the validation set: 33 \n",
      "Number of files in the test set: 33\n"
     ]
    }
   ],
   "source": [
    "tumor_dataset = TumorDataset(DATASET_PATH)\n",
    "tumor_dataset_aug = TumorDataset_Aug(DATASET_PATH)\n",
    "\n",
    "\n",
    "train_indices, validation_indices, test_indices = get_indices(len(tumor_dataset), val_split = VAL_SPLIT, test_split = TEST_SPLIT)\n",
    "train_sampler, validation_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(validation_indices), SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(tumor_dataset, BATCH_SIZE, sampler = train_sampler)\n",
    "trainloader_aug = torch.utils.data.DataLoader(tumor_dataset_aug, BATCH_SIZE, sampler = train_sampler)\n",
    "validationloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = validation_sampler)\n",
    "testloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = test_sampler)\n",
    "\n",
    "print('Number of files in the train set: %s \\nNumber of files in the validation set: %s \\nNumber of files in the test set: %s' \\\n",
    "      % (len(train_indices), len(validation_indices), len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run to save TorchIO augmented data. Don't run.\n",
    "\n",
    "# os.makedirs('./data/all_gbm_pre_reg_train_torchio_aug/', exist_ok = True)\n",
    "# os.makedirs('./data/all_tumors_reg_train_torchio_aug/', exist_ok = True)\n",
    "\n",
    "# trainloader_aug = torch.utils.data.DataLoader(tumor_dataset_aug, 1, sampler = train_sampler)\n",
    "# index = 0\n",
    "# for batch, data in enumerate(trainloader_aug):\n",
    "#     image = data['image'].view([155,240,240])\n",
    "#     mask = data['mask'].view([155,240,240])\n",
    "#     torch.save(image, './data/all_gbm_pre_reg_train_torchio_aug/'+str(index)+'.pt')\n",
    "#     torch.save(mask, './data/all_tumors_reg_train_torchio_aug/'+str(index)+'.pt')\n",
    "#     index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset_Aug(Dataset):\n",
    "    '''\n",
    "    Returns a TumorDataset class object which represents our tumor dataset.\n",
    "    TumorDataset inherits from torch.utils.data.Dataset class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, DEBUG = False):\n",
    "        '''\n",
    "        Constructor for our TumorDataset class.\n",
    "        Parameters:\n",
    "            root_dir(str): Directory with all the images.\n",
    "            DEBUG(bool): To switch to debug mode for image transformation.\n",
    "\n",
    "        Returns: None\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        # The default transformation is composed of \n",
    "        # 1) a grayscale conversion.\n",
    "        self.default_transformation = transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        self.DEBUG = DEBUG\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Overridden method from inheritted class to support\n",
    "        indexing of dataset such that datset[I] can be used\n",
    "        to get Ith sample.\n",
    "        Parameters:\n",
    "            index(int): Index of the dataset sample\n",
    "            \n",
    "        Return:\n",
    "            sample(dict): Contains the index, image, mask torch.Tensor.\n",
    "                        'index': Index of the image.\n",
    "                        'image': Contains the tumor image torch.Tensor.\n",
    "                        'mask' : Contains the mask image torch.Tensor.\n",
    "        '''\n",
    "        # Find the filenames for the tumor images and masks.\n",
    "        image_path = os.path.join(self.root_dir, \"all_gbm_pre_reg_train_torchio_aug\")\n",
    "        tumor_path = os.path.join(self.root_dir, \"all_tumors_reg_train_torchio_aug\")\n",
    "        \n",
    "        image_name = sorted(glob(os.path.join(image_path, \"*.pt\")))[index]\n",
    "        mask_name = sorted(glob(os.path.join(tumor_path, \"*.pt\")))[index]\n",
    "\n",
    "        # Use nibabel to open the images and masks.\n",
    "        image = torch.load(image_name)\n",
    "        mask = torch.load(mask_name)\n",
    "\n",
    "        # Apply the default transformations on the images and masks.\n",
    "        #image = self.default_transformation(image)\n",
    "        #mask = self.default_transformation(mask)\n",
    "\n",
    "        # Convert the images and masks to tensor.\n",
    "        #image = TF.to_tensor(image)\n",
    "        #mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Construct the images and masks together in the form of a dictionary.\n",
    "        sample = {'index': index, 'image': image, 'mask': mask}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Overridden method from inheritted class so that\n",
    "        len(self) returns the size of the dataset.\n",
    "        '''\n",
    "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
    "        total_image_files = len(glob(os.path.join(self.root_dir, 'all_gbm_pre_reg_train_torchio_aug', '*.pt')))\n",
    "        total_tumor_files = len(glob(os.path.join(self.root_dir, 'all_tumors_reg_train_torchio_aug', '*.pt')))\n",
    "\n",
    "        # Sanity check: the number of files shall be even since tumor images and masks are in pairs.\n",
    "        assert total_image_files == total_tumor_files, error_msg\n",
    "        \n",
    "        # Return how many image-mask pairs we have.\n",
    "        return total_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_dataset_aug = TumorDataset_Aug(DATASET_PATH)\n",
    "trainloader_aug = torch.utils.data.DataLoader(tumor_dataset_aug, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio\n",
    "from torchio.transforms import Rescale, RandomAffine\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some visualization functions\n",
    "\n",
    "def show_nifti(image_path_or_image, colormap='gray'):\n",
    "    try:\n",
    "        from niwidgets import NiftiWidget\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "            widget = NiftiWidget(image_path_or_image)\n",
    "            widget.nifti_plotter(colormap=colormap)\n",
    "    except Exception:\n",
    "        if isinstance(image_path_or_image, nib.AnalyzeImage):\n",
    "            nii = image_path_or_image\n",
    "        else:\n",
    "            image_path = image_path_or_image\n",
    "            nii = nib.load(str(image_path))\n",
    "        k = int(nii.shape[-1] / 2)\n",
    "        plt.imshow(nii.dataobj[..., k], cmap=colormap)\n",
    "\n",
    "def show_sample(sample, image_name, label_name=None):\n",
    "    if label_name is not None:\n",
    "        sample = copy.deepcopy(sample)\n",
    "        affine = sample[label_name][AFFINE]\n",
    "        label = sample[label_name][DATA][0].numpy().astype(np.uint8)\n",
    "        label_image = torchio.utils.nib_to_sitk(label, affine)\n",
    "        border = sitk.BinaryContour(label_image)\n",
    "        border_array, _ = torchio.utils.sitk_to_nib(border)\n",
    "        border_tensor = torch.from_numpy(border_array)\n",
    "        image_tensor = sample[image_name][DATA][0]\n",
    "        image_tensor[border_tensor > 0.5] = image_tensor.max()\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nii') as f:\n",
    "        torchio.ImagesDataset.save_sample(sample, {image_name: f.name})\n",
    "        show_nifti(f.name)\n",
    "\n",
    "def plot_histogram(axis, tensor, num_positions=100, label=None, alpha=0.05, color=None):\n",
    "    values = tensor.numpy().ravel()\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    positions = np.linspace(values.min(), values.max(), num=num_positions)\n",
    "    histogram = kernel(positions)\n",
    "    kwargs = dict(linewidth=1, color='black' if color is None else color, alpha=alpha)\n",
    "    if label is not None:\n",
    "        kwargs['label'] = label\n",
    "    axis.plot(positions, histogram, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 335 subjects\n"
     ]
    }
   ],
   "source": [
    "dataPath = Path('./data')\n",
    "\n",
    "t1_path = dataPath / 'all_gbm_pre_reg'\n",
    "tumor_path = dataPath / 'all_tumors_reg'\n",
    "\n",
    "t1_locations = sorted(t1_path.glob('*.nii.gz'))\n",
    "tumor_locations = sorted(tumor_path.glob('*.nii.gz'))\n",
    "\n",
    "\n",
    "assert len(t1_locations) == len(tumor_locations)\n",
    "\n",
    "subjects = []\n",
    "for (image_path, label_path) in zip(t1_locations, tumor_locations):\n",
    "    subject = torchio.Subject(\n",
    "        {'image': torchio.Image(image_path, torchio.INTENSITY),\n",
    "        'mask': torchio.Image(label_path, torchio.LABEL)}\n",
    "    )\n",
    "    subjects.append(subject)\n",
    "    \n",
    "dataset = torchio.ImagesDataset(subjects)\n",
    "print('Dataset size:', len(dataset), 'subjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "show_sample(sample, 'image', label_name='mask')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Spatial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Train a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------**if you have already trained the network offline, skip the blow three chunk**---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'Basic_Unet_best_model_0428_0.82.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([155, 240, 240])\n",
      "torch.Size([155, 240, 240])\n"
     ]
    }
   ],
   "source": [
    "print(tumor_dataset.__getitem__(0)['image'].size())\n",
    "print(tumor_dataset_aug.__getitem__(0)['image'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00653,  train score: 0.11157,  validation Loss: 0.02335,  validation score: 0.42149,  current lr:  0.0001 , Time: 458.09 s\tBest model saved at score: 0.42149\n",
      "Epoch:   2,  train Loss: 0.00467,  train score: 0.16090,  validation Loss: 0.02074,  validation score: 0.52495,  current lr:  0.0001 , Time: 457.56 s\tBest model saved at score: 0.52495\n",
      "Epoch:   3,  train Loss: 0.00381,  train score: 0.19057,  validation Loss: 0.01957,  validation score: 0.59440,  current lr:  0.0001 , Time: 504.29 s\tBest model saved at score: 0.59440\n",
      "Epoch:   4,  train Loss: 0.00331,  train score: 0.20656,  validation Loss: 0.01847,  validation score: 0.59740,  current lr:  0.0001 , Time: 488.47 s\tBest model saved at score: 0.59740\n",
      "Epoch:   5,  train Loss: 0.00301,  train score: 0.22841,  validation Loss: 0.01864,  validation score: 0.56573,  current lr:  0.0001 , Time: 464.19 s\n",
      "Epoch:   6,  train Loss: 0.00305,  train score: 0.23348,  validation Loss: 0.02171,  validation score: 0.54976,  current lr:  0.0001 , Time: 494.83 s\n",
      "Epoch     7: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   7,  train Loss: 0.00286,  train score: 0.24147,  validation Loss: 0.02090,  validation score: 0.53057,  current lr:  8.5e-05 , Time: 471.88 s\n",
      "Epoch:   8,  train Loss: 0.00268,  train score: 0.25503,  validation Loss: 0.02437,  validation score: 0.48921,  current lr:  8.5e-05 , Time: 474.80 s\n",
      "Epoch:   9,  train Loss: 0.00259,  train score: 0.25178,  validation Loss: 0.02403,  validation score: 0.56560,  current lr:  8.5e-05 , Time: 469.39 s\n",
      "Epoch    10: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:  10,  train Loss: 0.00241,  train score: 0.25790,  validation Loss: 0.02241,  validation score: 0.55628,  current lr:  7.225000000000001e-05 , Time: 463.77 s\n",
      "Epoch:  11,  train Loss: 0.00220,  train score: 0.27467,  validation Loss: 0.01867,  validation score: 0.58025,  current lr:  7.225000000000001e-05 , Time: 454.83 s\n",
      "Epoch:  12,  train Loss: 0.00221,  train score: 0.27285,  validation Loss: 0.02103,  validation score: 0.54804,  current lr:  7.225000000000001e-05 , Time: 483.41 s\n",
      "Epoch    13: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  13,  train Loss: 0.00220,  train score: 0.26627,  validation Loss: 0.01859,  validation score: 0.59773,  current lr:  6.141250000000001e-05 , Time: 457.23 s\tBest model saved at score: 0.59773\n",
      "Epoch:  14,  train Loss: 0.00204,  train score: 0.28598,  validation Loss: 0.01863,  validation score: 0.62878,  current lr:  6.141250000000001e-05 , Time: 480.97 s\tBest model saved at score: 0.62878\n",
      "Epoch:  15,  train Loss: 0.00181,  train score: 0.29329,  validation Loss: 0.01952,  validation score: 0.59249,  current lr:  6.141250000000001e-05 , Time: 462.92 s\n",
      "Epoch    16: reducing learning rate of group 0 to 5.2201e-05.\n",
      "Epoch:  16,  train Loss: 0.00176,  train score: 0.28921,  validation Loss: 0.02128,  validation score: 0.53102,  current lr:  5.2200625000000005e-05 , Time: 460.69 s\n",
      "Epoch:  17,  train Loss: 0.00187,  train score: 0.29358,  validation Loss: 0.02325,  validation score: 0.53944,  current lr:  5.2200625000000005e-05 , Time: 475.73 s\n",
      "Epoch:  18,  train Loss: 0.00195,  train score: 0.28726,  validation Loss: 0.02342,  validation score: 0.56335,  current lr:  5.2200625000000005e-05 , Time: 460.25 s\n",
      "Epoch    19: reducing learning rate of group 0 to 4.4371e-05.\n",
      "Epoch:  19,  train Loss: 0.00187,  train score: 0.29305,  validation Loss: 0.02006,  validation score: 0.60633,  current lr:  4.437053125e-05 , Time: 457.96 s\n",
      "Epoch:  20,  train Loss: 0.00181,  train score: 0.29100,  validation Loss: 0.01903,  validation score: 0.62848,  current lr:  4.437053125e-05 , Time: 468.68 s\n",
      "Epoch:  21,  train Loss: 0.00166,  train score: 0.29452,  validation Loss: 0.02107,  validation score: 0.61612,  current lr:  4.437053125e-05 , Time: 462.01 s\n",
      "Epoch    22: reducing learning rate of group 0 to 3.7715e-05.\n",
      "Epoch:  22,  train Loss: 0.00153,  train score: 0.29840,  validation Loss: 0.02406,  validation score: 0.57353,  current lr:  3.77149515625e-05 , Time: 458.38 s\n",
      "Epoch:  23,  train Loss: 0.00151,  train score: 0.30291,  validation Loss: 0.02225,  validation score: 0.56608,  current lr:  3.77149515625e-05 , Time: 469.44 s\n",
      "Epoch:  24,  train Loss: 0.00149,  train score: 0.30418,  validation Loss: 0.02003,  validation score: 0.62374,  current lr:  3.77149515625e-05 , Time: 458.16 s\n",
      "Epoch    25: reducing learning rate of group 0 to 3.2058e-05.\n",
      "Epoch:  25,  train Loss: 0.00148,  train score: 0.29943,  validation Loss: 0.02014,  validation score: 0.62402,  current lr:  3.2057708828124995e-05 , Time: 458.40 s\n",
      "Epoch:  26,  train Loss: 0.00151,  train score: 0.29945,  validation Loss: 0.02308,  validation score: 0.59284,  current lr:  3.2057708828124995e-05 , Time: 467.33 s\n",
      "Epoch:  27,  train Loss: 0.00142,  train score: 0.30249,  validation Loss: 0.02261,  validation score: 0.60901,  current lr:  3.2057708828124995e-05 , Time: 459.49 s\n",
      "Epoch    28: reducing learning rate of group 0 to 2.7249e-05.\n",
      "Epoch:  28,  train Loss: 0.00136,  train score: 0.30429,  validation Loss: 0.02038,  validation score: 0.63929,  current lr:  2.7249052503906245e-05 , Time: 459.99 s\tBest model saved at score: 0.63929\n",
      "Epoch:  29,  train Loss: 0.00132,  train score: 0.30653,  validation Loss: 0.01989,  validation score: 0.64503,  current lr:  2.7249052503906245e-05 , Time: 470.03 s\tBest model saved at score: 0.64503\n",
      "Epoch:  30,  train Loss: 0.00133,  train score: 0.30761,  validation Loss: 0.02035,  validation score: 0.63668,  current lr:  2.7249052503906245e-05 , Time: 460.50 s\n",
      "Epoch    31: reducing learning rate of group 0 to 2.3162e-05.\n",
      "Epoch:  31,  train Loss: 0.00131,  train score: 0.30752,  validation Loss: 0.02091,  validation score: 0.62870,  current lr:  2.3161694628320308e-05 , Time: 460.73 s\n",
      "Epoch:  32,  train Loss: 0.00134,  train score: 0.30707,  validation Loss: 0.01983,  validation score: 0.63586,  current lr:  2.3161694628320308e-05 , Time: 463.10 s\n",
      "Epoch:  33,  train Loss: 0.00132,  train score: 0.30889,  validation Loss: 0.01983,  validation score: 0.63246,  current lr:  2.3161694628320308e-05 , Time: 454.05 s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.9687e-05.\n",
      "Epoch:  34,  train Loss: 0.00131,  train score: 0.31037,  validation Loss: 0.01995,  validation score: 0.62770,  current lr:  1.9687440434072263e-05 , Time: 455.82 s\n",
      "Epoch:  35,  train Loss: 0.00129,  train score: 0.31112,  validation Loss: 0.02126,  validation score: 0.61191,  current lr:  1.9687440434072263e-05 , Time: 453.24 s\n",
      "Epoch:  36,  train Loss: 0.00128,  train score: 0.31201,  validation Loss: 0.02243,  validation score: 0.60168,  current lr:  1.9687440434072263e-05 , Time: 458.88 s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.6734e-05.\n",
      "Epoch:  37,  train Loss: 0.00127,  train score: 0.31263,  validation Loss: 0.02392,  validation score: 0.59041,  current lr:  1.673432436896142e-05 , Time: 459.58 s\n",
      "Epoch:  38,  train Loss: 0.00126,  train score: 0.31390,  validation Loss: 0.02544,  validation score: 0.56734,  current lr:  1.673432436896142e-05 , Time: 455.12 s\n",
      "Epoch:  39,  train Loss: 0.00125,  train score: 0.31431,  validation Loss: 0.02804,  validation score: 0.53971,  current lr:  1.673432436896142e-05 , Time: 466.41 s\n",
      "Epoch    40: reducing learning rate of group 0 to 1.4224e-05.\n",
      "Epoch:  40,  train Loss: 0.00123,  train score: 0.31509,  validation Loss: 0.03089,  validation score: 0.51890,  current lr:  1.4224175713617208e-05 , Time: 458.17 s\n",
      "Epoch:  41,  train Loss: 0.00124,  train score: 0.31541,  validation Loss: 0.03159,  validation score: 0.51731,  current lr:  1.4224175713617208e-05 , Time: 455.35 s\n",
      "Epoch:  42,  train Loss: 0.00125,  train score: 0.31405,  validation Loss: 0.03063,  validation score: 0.52442,  current lr:  1.4224175713617208e-05 , Time: 452.48 s\n",
      "Epoch    43: reducing learning rate of group 0 to 1.2091e-05.\n",
      "Epoch:  43,  train Loss: 0.00126,  train score: 0.31278,  validation Loss: 0.02929,  validation score: 0.53590,  current lr:  1.2090549356574626e-05 , Time: 457.89 s\n",
      "Epoch:  44,  train Loss: 0.00129,  train score: 0.31093,  validation Loss: 0.02535,  validation score: 0.58115,  current lr:  1.2090549356574626e-05 , Time: 454.01 s\n",
      "Epoch:  45,  train Loss: 0.00131,  train score: 0.30743,  validation Loss: 0.02280,  validation score: 0.61859,  current lr:  1.2090549356574626e-05 , Time: 454.71 s\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0277e-05.\n",
      "Epoch:  46,  train Loss: 0.00131,  train score: 0.30606,  validation Loss: 0.02190,  validation score: 0.63540,  current lr:  1.0276966953088432e-05 , Time: 460.74 s\n",
      "Epoch:  47,  train Loss: 0.00133,  train score: 0.30411,  validation Loss: 0.02183,  validation score: 0.64415,  current lr:  1.0276966953088432e-05 , Time: 459.03 s\n",
      "Epoch:  48,  train Loss: 0.00132,  train score: 0.30638,  validation Loss: 0.02144,  validation score: 0.64698,  current lr:  1.0276966953088432e-05 , Time: 452.91 s\tBest model saved at score: 0.64698\n",
      "Epoch    49: reducing learning rate of group 0 to 8.7354e-06.\n",
      "Epoch:  49,  train Loss: 0.00128,  train score: 0.30998,  validation Loss: 0.02059,  validation score: 0.65194,  current lr:  8.735421910125167e-06 , Time: 452.28 s\tBest model saved at score: 0.65194\n",
      "Epoch:  50,  train Loss: 0.00125,  train score: 0.31272,  validation Loss: 0.02000,  validation score: 0.65253,  current lr:  8.735421910125167e-06 , Time: 447.37 s\tBest model saved at score: 0.65253\n",
      "Epoch:  51,  train Loss: 0.00121,  train score: 0.31494,  validation Loss: 0.02045,  validation score: 0.64648,  current lr:  8.735421910125167e-06 , Time: 451.53 s\n",
      "Epoch    52: reducing learning rate of group 0 to 7.4251e-06.\n",
      "Epoch:  52,  train Loss: 0.00117,  train score: 0.31596,  validation Loss: 0.02112,  validation score: 0.63915,  current lr:  7.425108623606392e-06 , Time: 453.96 s\n",
      "Epoch:  53,  train Loss: 0.00115,  train score: 0.31664,  validation Loss: 0.02184,  validation score: 0.63186,  current lr:  7.425108623606392e-06 , Time: 457.41 s\n",
      "Epoch:  54,  train Loss: 0.00113,  train score: 0.31646,  validation Loss: 0.02226,  validation score: 0.63026,  current lr:  7.425108623606392e-06 , Time: 450.55 s\n",
      "Epoch    55: reducing learning rate of group 0 to 6.3113e-06.\n",
      "Epoch:  55,  train Loss: 0.00111,  train score: 0.31649,  validation Loss: 0.02254,  validation score: 0.63094,  current lr:  6.3113423300654325e-06 , Time: 453.02 s\n",
      "Epoch:  56,  train Loss: 0.00111,  train score: 0.31647,  validation Loss: 0.02284,  validation score: 0.63187,  current lr:  6.3113423300654325e-06 , Time: 460.31 s\n",
      "Epoch:  57,  train Loss: 0.00110,  train score: 0.31643,  validation Loss: 0.02308,  validation score: 0.63246,  current lr:  6.3113423300654325e-06 , Time: 455.61 s\n",
      "Epoch    58: reducing learning rate of group 0 to 5.3646e-06.\n",
      "Epoch:  58,  train Loss: 0.00109,  train score: 0.31661,  validation Loss: 0.02335,  validation score: 0.63237,  current lr:  5.3646409805556175e-06 , Time: 455.16 s\n",
      "Epoch:  59,  train Loss: 0.00109,  train score: 0.31670,  validation Loss: 0.02370,  validation score: 0.63143,  current lr:  5.3646409805556175e-06 , Time: 454.61 s\n",
      "Epoch:  60,  train Loss: 0.00109,  train score: 0.31695,  validation Loss: 0.02390,  validation score: 0.63108,  current lr:  5.3646409805556175e-06 , Time: 446.92 s\n",
      "Epoch    61: reducing learning rate of group 0 to 4.5599e-06.\n",
      "Epoch:  61,  train Loss: 0.00108,  train score: 0.31712,  validation Loss: 0.02404,  validation score: 0.63113,  current lr:  4.559944833472275e-06 , Time: 447.83 s\n",
      "Epoch:  62,  train Loss: 0.00108,  train score: 0.31724,  validation Loss: 0.02412,  validation score: 0.63128,  current lr:  4.559944833472275e-06 , Time: 448.61 s\n",
      "Epoch:  63,  train Loss: 0.00108,  train score: 0.31735,  validation Loss: 0.02412,  validation score: 0.63207,  current lr:  4.559944833472275e-06 , Time: 453.85 s\n",
      "Epoch    64: reducing learning rate of group 0 to 3.8760e-06.\n",
      "Epoch:  64,  train Loss: 0.00107,  train score: 0.31748,  validation Loss: 0.02411,  validation score: 0.63303,  current lr:  3.875953108451433e-06 , Time: 447.08 s\n",
      "Epoch:  65,  train Loss: 0.00107,  train score: 0.31760,  validation Loss: 0.02401,  validation score: 0.63417,  current lr:  3.875953108451433e-06 , Time: 448.05 s\n",
      "Epoch:  66,  train Loss: 0.00107,  train score: 0.31774,  validation Loss: 0.02399,  validation score: 0.63482,  current lr:  3.875953108451433e-06 , Time: 461.18 s\n",
      "Epoch    67: reducing learning rate of group 0 to 3.2946e-06.\n",
      "Epoch:  67,  train Loss: 0.00106,  train score: 0.31790,  validation Loss: 0.02399,  validation score: 0.63528,  current lr:  3.2945601421837183e-06 , Time: 451.74 s\n",
      "Epoch:  68,  train Loss: 0.00106,  train score: 0.31807,  validation Loss: 0.02397,  validation score: 0.63553,  current lr:  3.2945601421837183e-06 , Time: 451.49 s\n",
      "Epoch:  69,  train Loss: 0.00106,  train score: 0.31823,  validation Loss: 0.02402,  validation score: 0.63548,  current lr:  3.2945601421837183e-06 , Time: 459.35 s\n",
      "Epoch    70: reducing learning rate of group 0 to 2.8004e-06.\n",
      "Epoch:  70,  train Loss: 0.00106,  train score: 0.31838,  validation Loss: 0.02409,  validation score: 0.63524,  current lr:  2.8003761208561607e-06 , Time: 451.67 s\n",
      "Training Finished after 70 epoches\n"
     ]
    }
   ],
   "source": [
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "            \n",
    "#     for batch, data in enumerate(trainloader):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    for batch, data in enumerate(trainloader_aug):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'torchio_rand_aug_model.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The previous training was offline augmentation, now for online augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "\n",
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'torchio_rand_aug_model.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define online augmentation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an online augmentation function\n",
    "\n",
    "def ol_aug(data): \n",
    "    # data is a class 'dict' from trainlaoder\n",
    "    spatial_augmentation_type = np.random.choice(4,1, p = [0.1, 0.4, 0.25, 0.25]).item()\n",
    "        # no transform, RandomNoise, RandomBiasField, RandomMotion, RandomGhosting\n",
    "        # For now only introduce random Gaussian noise\n",
    "    intensity_augmentation_type = np.random.choice(4,1, p = [0.05, 0.48, 0.36, 0.11]).item()\n",
    "\n",
    "    if spatial_augmentation_type == 1:\n",
    "        spatial_transform = RandomAffine(scales=(0.9,1.1),degrees=(-1,1),\n",
    "                                        image_interpolation=Interpolation.NEAREST,seed=seed)\n",
    "    elif spatial_augmentation_type == 2:\n",
    "        spatial_transform = RandomElasticDeformation(num_control_points=20,locked_borders=2,\n",
    "                                                    proportion_to_augment=1, \n",
    "                                                    image_interpolation=Interpolation.NEAREST,\n",
    "                                                    seed=seed) \n",
    "                                                     #deformation_std = 50)\n",
    "    elif spatial_augmentation_type == 3:\n",
    "        axis = np.random.choice([0,2],1).item()\n",
    "        spatial_transform = RandomFlip(seed = seed, flip_probability = 1, axes = axis)\n",
    "\n",
    "    if intensity_augmentation_type == 1: \n",
    "        intensity_transform = RandomNoise(seed = seed, std = (0, 5))\n",
    "    elif intensity_augmentation_type == 2: \n",
    "        intensity_transform = RandomBlur(std=(0, 0.5),seed=seed)\n",
    "    elif intensity_augmentation_type == 3:\n",
    "            intensity_transform = RandomGhosting(seed = seed, \n",
    "                                                 proportion_to_augment = 1, \n",
    "                                                 num_ghosts = (1, 3))\n",
    "\n",
    "    # Apply spatial transform. \n",
    "    # spatial_transformed_subject = spatial_transform(subject) if spatial_augmentation_type > 0 else subject\n",
    "    transformed_data = spatial_transform(data) if spatial_augmentation_type > 0 else data\n",
    "    transformed_data = intensity_transform(transformed_data) if intensity_augmentation_type > 0 else transformed_data\n",
    "\n",
    "    data = transformed_data\n",
    "    \n",
    "    return data\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00237,  train score: 0.41602,  validation Loss: 0.01989,  validation score: 0.59142,  current lr:  0.0001 , Time: 484.88 s\tBest model saved at score: 0.59142\n",
      "Epoch:   2,  train Loss: 0.00178,  train score: 0.40789,  validation Loss: 0.01764,  validation score: 0.63579,  current lr:  0.0001 , Time: 482.19 s\tBest model saved at score: 0.63579\n",
      "Epoch:   3,  train Loss: 0.00154,  train score: 0.42204,  validation Loss: 0.01542,  validation score: 0.68897,  current lr:  0.0001 , Time: 484.84 s\tBest model saved at score: 0.68897\n",
      "Epoch:   4,  train Loss: 0.00154,  train score: 0.42304,  validation Loss: 0.01771,  validation score: 0.65277,  current lr:  0.0001 , Time: 484.57 s\n",
      "Epoch:   5,  train Loss: 0.00148,  train score: 0.42251,  validation Loss: 0.01574,  validation score: 0.66675,  current lr:  0.0001 , Time: 483.17 s\n",
      "Epoch:   6,  train Loss: 0.00132,  train score: 0.36893,  validation Loss: 0.01460,  validation score: 0.70379,  current lr:  0.0001 , Time: 481.60 s\tBest model saved at score: 0.70379\n",
      "Epoch:   7,  train Loss: 0.00123,  train score: 0.43466,  validation Loss: 0.01590,  validation score: 0.67323,  current lr:  0.0001 , Time: 481.69 s\n",
      "Epoch:   8,  train Loss: 0.00121,  train score: 0.43409,  validation Loss: 0.01533,  validation score: 0.68085,  current lr:  0.0001 , Time: 483.37 s\n",
      "Epoch     9: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   9,  train Loss: 0.00149,  train score: 0.42299,  validation Loss: 0.01541,  validation score: 0.69823,  current lr:  8.5e-05 , Time: 481.04 s\n",
      "Epoch:  10,  train Loss: 0.00172,  train score: 0.42427,  validation Loss: 0.02068,  validation score: 0.59622,  current lr:  8.5e-05 , Time: 481.89 s\n",
      "Epoch:  11,  train Loss: 0.00160,  train score: 0.43041,  validation Loss: 0.01687,  validation score: 0.65976,  current lr:  8.5e-05 , Time: 482.65 s\n",
      "Epoch    12: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:  12,  train Loss: 0.00158,  train score: 0.42598,  validation Loss: 0.01601,  validation score: 0.64439,  current lr:  7.225000000000001e-05 , Time: 481.75 s\n",
      "Epoch:  13,  train Loss: 0.00149,  train score: 0.42914,  validation Loss: 0.01798,  validation score: 0.63419,  current lr:  7.225000000000001e-05 , Time: 480.31 s\n",
      "Epoch:  14,  train Loss: 0.00130,  train score: 0.42824,  validation Loss: 0.01580,  validation score: 0.67831,  current lr:  7.225000000000001e-05 , Time: 478.86 s\n",
      "Epoch    15: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  15,  train Loss: 0.00115,  train score: 0.43460,  validation Loss: 0.01611,  validation score: 0.68638,  current lr:  6.141250000000001e-05 , Time: 477.56 s\n",
      "Epoch:  16,  train Loss: 0.00110,  train score: 0.43742,  validation Loss: 0.01648,  validation score: 0.68463,  current lr:  6.141250000000001e-05 , Time: 478.68 s\n",
      "Epoch:  17,  train Loss: 0.00108,  train score: 0.43495,  validation Loss: 0.01622,  validation score: 0.68637,  current lr:  6.141250000000001e-05 , Time: 478.76 s\n",
      "Epoch    18: reducing learning rate of group 0 to 5.2201e-05.\n",
      "Epoch:  18,  train Loss: 0.00104,  train score: 0.44014,  validation Loss: 0.01676,  validation score: 0.67909,  current lr:  5.2200625000000005e-05 , Time: 481.21 s\n",
      "Epoch:  19,  train Loss: 0.00104,  train score: 0.44072,  validation Loss: 0.01598,  validation score: 0.68497,  current lr:  5.2200625000000005e-05 , Time: 480.98 s\n",
      "Epoch:  20,  train Loss: 0.00102,  train score: 0.37908,  validation Loss: 0.01661,  validation score: 0.69061,  current lr:  5.2200625000000005e-05 , Time: 482.03 s\n",
      "Epoch    21: reducing learning rate of group 0 to 4.4371e-05.\n",
      "Epoch:  21,  train Loss: 0.00099,  train score: 0.43813,  validation Loss: 0.01700,  validation score: 0.68340,  current lr:  4.437053125e-05 , Time: 483.33 s\n",
      "Epoch:  22,  train Loss: 0.00098,  train score: 0.43848,  validation Loss: 0.01748,  validation score: 0.67860,  current lr:  4.437053125e-05 , Time: 481.86 s\n",
      "Epoch:  23,  train Loss: 0.00100,  train score: 0.43992,  validation Loss: 0.01854,  validation score: 0.64494,  current lr:  4.437053125e-05 , Time: 479.51 s\n",
      "Epoch    24: reducing learning rate of group 0 to 3.7715e-05.\n",
      "Epoch:  24,  train Loss: 0.00112,  train score: 0.43709,  validation Loss: 0.01669,  validation score: 0.68504,  current lr:  3.77149515625e-05 , Time: 478.79 s\n",
      "Epoch:  25,  train Loss: 0.00111,  train score: 0.44027,  validation Loss: 0.01603,  validation score: 0.69525,  current lr:  3.77149515625e-05 , Time: 480.97 s\n",
      "Epoch:  26,  train Loss: 0.00104,  train score: 0.44358,  validation Loss: 0.01689,  validation score: 0.68257,  current lr:  3.77149515625e-05 , Time: 484.18 s\n",
      "Epoch    27: reducing learning rate of group 0 to 3.2058e-05.\n",
      "Epoch:  27,  train Loss: 0.00101,  train score: 0.44264,  validation Loss: 0.01653,  validation score: 0.69555,  current lr:  3.2057708828124995e-05 , Time: 480.54 s\n",
      "Epoch:  28,  train Loss: 0.00102,  train score: 0.43969,  validation Loss: 0.01689,  validation score: 0.69453,  current lr:  3.2057708828124995e-05 , Time: 480.54 s\n",
      "Epoch:  29,  train Loss: 0.00102,  train score: 0.43864,  validation Loss: 0.01680,  validation score: 0.69239,  current lr:  3.2057708828124995e-05 , Time: 480.32 s\n",
      "Epoch    30: reducing learning rate of group 0 to 2.7249e-05.\n",
      "Epoch:  30,  train Loss: 0.00096,  train score: 0.44385,  validation Loss: 0.01675,  validation score: 0.69771,  current lr:  2.7249052503906245e-05 , Time: 480.50 s\n",
      "Epoch:  31,  train Loss: 0.00094,  train score: 0.43975,  validation Loss: 0.01762,  validation score: 0.68752,  current lr:  2.7249052503906245e-05 , Time: 482.44 s\n",
      "Epoch:  32,  train Loss: 0.00093,  train score: 0.44496,  validation Loss: 0.01749,  validation score: 0.69262,  current lr:  2.7249052503906245e-05 , Time: 481.24 s\n",
      "Epoch    33: reducing learning rate of group 0 to 2.3162e-05.\n",
      "Epoch:  33,  train Loss: 0.00092,  train score: 0.44073,  validation Loss: 0.01810,  validation score: 0.68461,  current lr:  2.3161694628320308e-05 , Time: 479.98 s\n",
      "Epoch:  34,  train Loss: 0.00091,  train score: 0.44109,  validation Loss: 0.01870,  validation score: 0.67794,  current lr:  2.3161694628320308e-05 , Time: 481.33 s\n",
      "Epoch:  35,  train Loss: 0.00091,  train score: 0.44501,  validation Loss: 0.01813,  validation score: 0.68851,  current lr:  2.3161694628320308e-05 , Time: 482.04 s\n",
      "Epoch    36: reducing learning rate of group 0 to 1.9687e-05.\n",
      "Epoch:  36,  train Loss: 0.00090,  train score: 0.44558,  validation Loss: 0.01819,  validation score: 0.68745,  current lr:  1.9687440434072263e-05 , Time: 479.74 s\n",
      "Epoch:  37,  train Loss: 0.00089,  train score: 0.44124,  validation Loss: 0.01851,  validation score: 0.68700,  current lr:  1.9687440434072263e-05 , Time: 481.18 s\n",
      "Epoch:  38,  train Loss: 0.00088,  train score: 0.44150,  validation Loss: 0.01867,  validation score: 0.68595,  current lr:  1.9687440434072263e-05 , Time: 480.00 s\n",
      "Epoch    39: reducing learning rate of group 0 to 1.6734e-05.\n",
      "Epoch:  39,  train Loss: 0.00088,  train score: 0.43050,  validation Loss: 0.01874,  validation score: 0.68579,  current lr:  1.673432436896142e-05 , Time: 480.15 s\n",
      "Epoch:  40,  train Loss: 0.00087,  train score: 0.44679,  validation Loss: 0.01909,  validation score: 0.68316,  current lr:  1.673432436896142e-05 , Time: 479.60 s\n",
      "Epoch:  41,  train Loss: 0.00087,  train score: 0.44257,  validation Loss: 0.01908,  validation score: 0.68538,  current lr:  1.673432436896142e-05 , Time: 482.51 s\n",
      "Epoch    42: reducing learning rate of group 0 to 1.4224e-05.\n",
      "Epoch:  42,  train Loss: 0.00087,  train score: 0.44727,  validation Loss: 0.01938,  validation score: 0.68399,  current lr:  1.4224175713617208e-05 , Time: 482.01 s\n",
      "Epoch:  43,  train Loss: 0.00086,  train score: 0.44290,  validation Loss: 0.01940,  validation score: 0.68405,  current lr:  1.4224175713617208e-05 , Time: 479.88 s\n",
      "Epoch:  44,  train Loss: 0.00086,  train score: 0.44305,  validation Loss: 0.01954,  validation score: 0.68290,  current lr:  1.4224175713617208e-05 , Time: 480.43 s\n",
      "Epoch    45: reducing learning rate of group 0 to 1.2091e-05.\n",
      "Epoch:  45,  train Loss: 0.00085,  train score: 0.38493,  validation Loss: 0.01963,  validation score: 0.68454,  current lr:  1.2090549356574626e-05 , Time: 483.21 s\n",
      "Epoch:  46,  train Loss: 0.00085,  train score: 0.44329,  validation Loss: 0.01978,  validation score: 0.68264,  current lr:  1.2090549356574626e-05 , Time: 480.75 s\n",
      "Epoch:  47,  train Loss: 0.00085,  train score: 0.44794,  validation Loss: 0.01993,  validation score: 0.68324,  current lr:  1.2090549356574626e-05 , Time: 481.21 s\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0277e-05.\n",
      "Epoch:  48,  train Loss: 0.00085,  train score: 0.44813,  validation Loss: 0.02004,  validation score: 0.68271,  current lr:  1.0276966953088432e-05 , Time: 480.33 s\n",
      "Epoch:  49,  train Loss: 0.00084,  train score: 0.44811,  validation Loss: 0.02025,  validation score: 0.68174,  current lr:  1.0276966953088432e-05 , Time: 480.78 s\n",
      "Epoch:  50,  train Loss: 0.00084,  train score: 0.44357,  validation Loss: 0.02024,  validation score: 0.68192,  current lr:  1.0276966953088432e-05 , Time: 480.72 s\n",
      "Epoch    51: reducing learning rate of group 0 to 8.7354e-06.\n",
      "Epoch:  51,  train Loss: 0.00084,  train score: 0.44833,  validation Loss: 0.02041,  validation score: 0.68152,  current lr:  8.735421910125167e-06 , Time: 480.77 s\n",
      "Epoch:  52,  train Loss: 0.00083,  train score: 0.44847,  validation Loss: 0.02047,  validation score: 0.68185,  current lr:  8.735421910125167e-06 , Time: 480.58 s\n",
      "Epoch:  53,  train Loss: 0.00083,  train score: 0.44845,  validation Loss: 0.02059,  validation score: 0.68139,  current lr:  8.735421910125167e-06 , Time: 479.98 s\n",
      "Epoch    54: reducing learning rate of group 0 to 7.4251e-06.\n",
      "Epoch:  54,  train Loss: 0.00083,  train score: 0.44405,  validation Loss: 0.02063,  validation score: 0.68096,  current lr:  7.425108623606392e-06 , Time: 480.80 s\n",
      "Epoch:  55,  train Loss: 0.00083,  train score: 0.44873,  validation Loss: 0.02076,  validation score: 0.68096,  current lr:  7.425108623606392e-06 , Time: 480.68 s\n",
      "Epoch:  56,  train Loss: 0.00083,  train score: 0.44424,  validation Loss: 0.02079,  validation score: 0.68076,  current lr:  7.425108623606392e-06 , Time: 482.53 s\n",
      "Epoch    57: reducing learning rate of group 0 to 6.3113e-06.\n",
      "Epoch:  57,  train Loss: 0.00082,  train score: 0.38605,  validation Loss: 0.02089,  validation score: 0.68132,  current lr:  6.3113423300654325e-06 , Time: 482.29 s\n",
      "Epoch:  58,  train Loss: 0.00082,  train score: 0.44902,  validation Loss: 0.02099,  validation score: 0.68042,  current lr:  6.3113423300654325e-06 , Time: 482.05 s\n",
      "Epoch:  59,  train Loss: 0.00082,  train score: 0.44441,  validation Loss: 0.02104,  validation score: 0.68030,  current lr:  6.3113423300654325e-06 , Time: 481.58 s\n",
      "Epoch    60: reducing learning rate of group 0 to 5.3646e-06.\n",
      "Epoch:  60,  train Loss: 0.00082,  train score: 0.44448,  validation Loss: 0.02105,  validation score: 0.68045,  current lr:  5.3646409805556175e-06 , Time: 482.62 s\n",
      "Epoch:  61,  train Loss: 0.00082,  train score: 0.44452,  validation Loss: 0.02110,  validation score: 0.68057,  current lr:  5.3646409805556175e-06 , Time: 481.18 s\n",
      "Epoch:  62,  train Loss: 0.00081,  train score: 0.44461,  validation Loss: 0.02119,  validation score: 0.68014,  current lr:  5.3646409805556175e-06 , Time: 481.30 s\n",
      "Epoch    63: reducing learning rate of group 0 to 4.5599e-06.\n",
      "Epoch:  63,  train Loss: 0.00081,  train score: 0.38646,  validation Loss: 0.02135,  validation score: 0.68017,  current lr:  4.559944833472275e-06 , Time: 481.99 s\n",
      "Epoch:  64,  train Loss: 0.00081,  train score: 0.44939,  validation Loss: 0.02145,  validation score: 0.67943,  current lr:  4.559944833472275e-06 , Time: 482.53 s\n",
      "Epoch:  65,  train Loss: 0.00081,  train score: 0.44936,  validation Loss: 0.02155,  validation score: 0.67907,  current lr:  4.559944833472275e-06 , Time: 482.30 s\n",
      "Epoch    66: reducing learning rate of group 0 to 3.8760e-06.\n",
      "Epoch:  66,  train Loss: 0.00081,  train score: 0.44942,  validation Loss: 0.02162,  validation score: 0.67893,  current lr:  3.875953108451433e-06 , Time: 481.39 s\n",
      "Epoch:  67,  train Loss: 0.00081,  train score: 0.44487,  validation Loss: 0.02161,  validation score: 0.67909,  current lr:  3.875953108451433e-06 , Time: 480.50 s\n",
      "Epoch:  68,  train Loss: 0.00081,  train score: 0.44956,  validation Loss: 0.02173,  validation score: 0.67874,  current lr:  3.875953108451433e-06 , Time: 480.56 s\n",
      "Epoch    69: reducing learning rate of group 0 to 3.2946e-06.\n",
      "Epoch:  69,  train Loss: 0.00081,  train score: 0.44957,  validation Loss: 0.02180,  validation score: 0.67856,  current lr:  3.2945601421837183e-06 , Time: 481.84 s\n",
      "Epoch:  70,  train Loss: 0.00080,  train score: 0.44960,  validation Loss: 0.02184,  validation score: 0.67864,  current lr:  3.2945601421837183e-06 , Time: 478.84 s\n",
      "Epoch:  71,  train Loss: 0.00080,  train score: 0.44508,  validation Loss: 0.02183,  validation score: 0.67869,  current lr:  3.2945601421837183e-06 , Time: 481.74 s\n",
      "Epoch    72: reducing learning rate of group 0 to 2.8004e-06.\n",
      "Epoch:  72,  train Loss: 0.00080,  train score: 0.44971,  validation Loss: 0.02194,  validation score: 0.67847,  current lr:  2.8003761208561607e-06 , Time: 481.40 s\n",
      "Epoch:  73,  train Loss: 0.00080,  train score: 0.44974,  validation Loss: 0.02201,  validation score: 0.67824,  current lr:  2.8003761208561607e-06 , Time: 482.35 s\n",
      "Epoch:  74,  train Loss: 0.00080,  train score: 0.44521,  validation Loss: 0.02198,  validation score: 0.67841,  current lr:  2.8003761208561607e-06 , Time: 482.46 s\n",
      "Epoch    75: reducing learning rate of group 0 to 2.3803e-06.\n",
      "Epoch:  75,  train Loss: 0.00080,  train score: 0.44523,  validation Loss: 0.02202,  validation score: 0.67826,  current lr:  2.3803197027277364e-06 , Time: 481.62 s\n",
      "Epoch:  76,  train Loss: 0.00080,  train score: 0.38699,  validation Loss: 0.02213,  validation score: 0.67808,  current lr:  2.3803197027277364e-06 , Time: 479.90 s\n",
      "Epoch:  77,  train Loss: 0.00080,  train score: 0.44991,  validation Loss: 0.02218,  validation score: 0.67782,  current lr:  2.3803197027277364e-06 , Time: 482.48 s\n",
      "Epoch    78: reducing learning rate of group 0 to 2.0233e-06.\n",
      "Epoch:  78,  train Loss: 0.00080,  train score: 0.44990,  validation Loss: 0.02223,  validation score: 0.67774,  current lr:  2.0232717473185757e-06 , Time: 480.94 s\n",
      "Epoch:  79,  train Loss: 0.00080,  train score: 0.44534,  validation Loss: 0.02220,  validation score: 0.67793,  current lr:  2.0232717473185757e-06 , Time: 482.80 s\n",
      "Epoch:  80,  train Loss: 0.00080,  train score: 0.44535,  validation Loss: 0.02224,  validation score: 0.67782,  current lr:  2.0232717473185757e-06 , Time: 484.40 s\n",
      "Training Finished after 80 epoches\n"
     ]
    }
   ],
   "source": [
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "#     for batch, data in enumerate(trainloader_aug):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'torchio_ol_aug_model.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# online augmentation works better than offline approach, but still not enough, now to test a hypothesis, we will remove the affline and retrain the model (continue with the last one)\n",
    "\n",
    "### also slight modification to the augmentation probability of different method; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------online augmentation without offline----Don't FORGET to RUN----------\n",
    "(note that the loaded model was still trained with offline which has affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an online augmentation function, but without affine augmentation\n",
    "\n",
    "def ol_aug_noAffine(data): \n",
    "    # data is a class 'dict' from trainlaoder\n",
    "    spatial_augmentation_type = np.random.choice(3,1, p = [0.1, 0.45, 0.45]).item()\n",
    "        # no transform, RandomNoise, RandomBiasField, RandomMotion, RandomGhosting\n",
    "        # For now only introduce random Gaussian noise\n",
    "    intensity_augmentation_type = np.random.choice(4,1, p = [0.05, 0.32, 0.32, 0.31]).item()\n",
    "\n",
    "#     if spatial_augmentation_type == 1:\n",
    "#         spatial_transform = RandomAffine(scales=(0.9,1.1),degrees=(-1,1),\n",
    "#                                         image_interpolation=Interpolation.NEAREST,seed=seed)\n",
    "    if spatial_augmentation_type == 1:\n",
    "        spatial_transform = RandomElasticDeformation(num_control_points=20,locked_borders=2,\n",
    "                                                     proportion_to_augment=1, \n",
    "                                                     image_interpolation=Interpolation.NEAREST,\n",
    "                                                     seed=seed) \n",
    "                                                     #deformation_std = 50)\n",
    "    elif spatial_augmentation_type == 2:\n",
    "        axis = np.random.choice([0,2],1).item()\n",
    "        spatial_transform = RandomFlip(seed = seed, flip_probability = 1, axes = axis)\n",
    "\n",
    "    if intensity_augmentation_type == 1: \n",
    "        intensity_transform = RandomNoise(seed = seed, std = (0, 5))\n",
    "    elif intensity_augmentation_type == 2: \n",
    "        intensity_transform = RandomBlur(std=(0, 0.5),seed=seed)\n",
    "    elif intensity_augmentation_type == 3:\n",
    "        intensity_transform = RandomGhosting(seed = seed, \n",
    "                                                 proportion_to_augment = 1, \n",
    "                                                 num_ghosts = (1, 3))\n",
    "\n",
    "    # Apply spatial transform. \n",
    "    # spatial_transformed_subject = spatial_transform(subject) if spatial_augmentation_type > 0 else subject\n",
    "    transformed_data = spatial_transform(data) if spatial_augmentation_type > 0 else data\n",
    "    transformed_data = intensity_transform(transformed_data) if intensity_augmentation_type > 0 else transformed_data\n",
    "\n",
    "    data = transformed_data\n",
    "    \n",
    "    return data\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "# loading the previous best model (from online augmentation)\n",
    "# for the new online augmentation without affine and different probability\n",
    "\n",
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'torchio_ol_aug_model.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00153,  train score: 0.35056,  validation Loss: 0.01842,  validation score: 0.62575,  current lr:  0.0001 , Time: 473.52 s\tBest model saved at score: 0.62575\n",
      "Epoch:   2,  train Loss: 0.00168,  train score: 0.42599,  validation Loss: 0.01805,  validation score: 0.60882,  current lr:  0.0001 , Time: 473.00 s\n",
      "Epoch:   3,  train Loss: 0.00146,  train score: 0.42452,  validation Loss: 0.01541,  validation score: 0.67095,  current lr:  0.0001 , Time: 472.96 s\tBest model saved at score: 0.67095\n",
      "Epoch:   4,  train Loss: 0.00137,  train score: 0.36935,  validation Loss: 0.01497,  validation score: 0.68496,  current lr:  0.0001 , Time: 472.14 s\tBest model saved at score: 0.68496\n",
      "Epoch:   5,  train Loss: 0.00131,  train score: 0.43158,  validation Loss: 0.01541,  validation score: 0.68212,  current lr:  0.0001 , Time: 471.31 s\n",
      "Epoch:   6,  train Loss: 0.00126,  train score: 0.43658,  validation Loss: 0.01523,  validation score: 0.68875,  current lr:  0.0001 , Time: 473.25 s\tBest model saved at score: 0.68875\n",
      "Epoch     7: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   7,  train Loss: 0.00126,  train score: 0.43636,  validation Loss: 0.01872,  validation score: 0.64433,  current lr:  8.5e-05 , Time: 473.43 s\n",
      "Epoch:   8,  train Loss: 0.00141,  train score: 0.43150,  validation Loss: 0.01635,  validation score: 0.67132,  current lr:  8.5e-05 , Time: 474.20 s\n",
      "Epoch:   9,  train Loss: 0.00139,  train score: 0.36505,  validation Loss: 0.01724,  validation score: 0.66853,  current lr:  8.5e-05 , Time: 472.38 s\n",
      "Epoch    10: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:  10,  train Loss: 0.00127,  train score: 0.37061,  validation Loss: 0.01605,  validation score: 0.67866,  current lr:  7.225000000000001e-05 , Time: 472.43 s\n",
      "Epoch:  11,  train Loss: 0.00120,  train score: 0.43477,  validation Loss: 0.01537,  validation score: 0.69979,  current lr:  7.225000000000001e-05 , Time: 472.48 s\tBest model saved at score: 0.69979\n",
      "Epoch:  12,  train Loss: 0.00109,  train score: 0.43825,  validation Loss: 0.01545,  validation score: 0.69297,  current lr:  7.225000000000001e-05 , Time: 471.31 s\n",
      "Epoch    13: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  13,  train Loss: 0.00107,  train score: 0.43509,  validation Loss: 0.01794,  validation score: 0.66179,  current lr:  6.141250000000001e-05 , Time: 473.62 s\n",
      "Epoch:  14,  train Loss: 0.00106,  train score: 0.43998,  validation Loss: 0.01513,  validation score: 0.70536,  current lr:  6.141250000000001e-05 , Time: 472.38 s\tBest model saved at score: 0.70536\n",
      "Epoch:  15,  train Loss: 0.00101,  train score: 0.37843,  validation Loss: 0.01564,  validation score: 0.70797,  current lr:  6.141250000000001e-05 , Time: 470.80 s\tBest model saved at score: 0.70797\n",
      "Epoch    16: reducing learning rate of group 0 to 5.2201e-05.\n",
      "Epoch:  16,  train Loss: 0.00099,  train score: 0.43933,  validation Loss: 0.01633,  validation score: 0.69588,  current lr:  5.2200625000000005e-05 , Time: 471.72 s\n",
      "Epoch:  17,  train Loss: 0.00097,  train score: 0.44177,  validation Loss: 0.01607,  validation score: 0.69957,  current lr:  5.2200625000000005e-05 , Time: 471.94 s\n",
      "Epoch:  18,  train Loss: 0.00097,  train score: 0.44280,  validation Loss: 0.01699,  validation score: 0.68794,  current lr:  5.2200625000000005e-05 , Time: 470.64 s\n",
      "Epoch    19: reducing learning rate of group 0 to 4.4371e-05.\n",
      "Epoch:  19,  train Loss: 0.00097,  train score: 0.38240,  validation Loss: 0.01567,  validation score: 0.71269,  current lr:  4.437053125e-05 , Time: 469.97 s\tBest model saved at score: 0.71269\n",
      "Epoch:  20,  train Loss: 0.00095,  train score: 0.44467,  validation Loss: 0.01693,  validation score: 0.69097,  current lr:  4.437053125e-05 , Time: 471.20 s\n",
      "Epoch:  21,  train Loss: 0.00094,  train score: 0.44092,  validation Loss: 0.01695,  validation score: 0.70250,  current lr:  4.437053125e-05 , Time: 468.11 s\n",
      "Epoch    22: reducing learning rate of group 0 to 3.7715e-05.\n",
      "Epoch:  22,  train Loss: 0.00092,  train score: 0.44509,  validation Loss: 0.01700,  validation score: 0.69352,  current lr:  3.77149515625e-05 , Time: 471.39 s\n",
      "Epoch:  23,  train Loss: 0.00093,  train score: 0.44452,  validation Loss: 0.01666,  validation score: 0.70367,  current lr:  3.77149515625e-05 , Time: 470.13 s\n",
      "Epoch:  24,  train Loss: 0.00092,  train score: 0.44207,  validation Loss: 0.01741,  validation score: 0.69692,  current lr:  3.77149515625e-05 , Time: 469.00 s\n",
      "Epoch    25: reducing learning rate of group 0 to 3.2058e-05.\n",
      "Epoch:  25,  train Loss: 0.00090,  train score: 0.38320,  validation Loss: 0.01726,  validation score: 0.70161,  current lr:  3.2057708828124995e-05 , Time: 471.44 s\n",
      "Epoch:  26,  train Loss: 0.00090,  train score: 0.38413,  validation Loss: 0.01699,  validation score: 0.70553,  current lr:  3.2057708828124995e-05 , Time: 470.34 s\n",
      "Epoch:  27,  train Loss: 0.00093,  train score: 0.38134,  validation Loss: 0.01659,  validation score: 0.70455,  current lr:  3.2057708828124995e-05 , Time: 470.46 s\n",
      "Epoch    28: reducing learning rate of group 0 to 2.7249e-05.\n",
      "Epoch:  28,  train Loss: 0.00101,  train score: 0.38233,  validation Loss: 0.01652,  validation score: 0.69653,  current lr:  2.7249052503906245e-05 , Time: 468.90 s\n",
      "Epoch:  29,  train Loss: 0.00106,  train score: 0.38021,  validation Loss: 0.01694,  validation score: 0.67359,  current lr:  2.7249052503906245e-05 , Time: 469.71 s\n",
      "Epoch:  30,  train Loss: 0.00095,  train score: 0.43946,  validation Loss: 0.01675,  validation score: 0.70300,  current lr:  2.7249052503906245e-05 , Time: 469.71 s\n",
      "Epoch    31: reducing learning rate of group 0 to 2.3162e-05.\n",
      "Epoch:  31,  train Loss: 0.00090,  train score: 0.38357,  validation Loss: 0.01715,  validation score: 0.69848,  current lr:  2.3161694628320308e-05 , Time: 468.59 s\n",
      "Epoch:  32,  train Loss: 0.00087,  train score: 0.44638,  validation Loss: 0.01738,  validation score: 0.70113,  current lr:  2.3161694628320308e-05 , Time: 469.44 s\n",
      "Epoch:  33,  train Loss: 0.00086,  train score: 0.44284,  validation Loss: 0.01788,  validation score: 0.69796,  current lr:  2.3161694628320308e-05 , Time: 468.60 s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.9687e-05.\n",
      "Epoch:  34,  train Loss: 0.00085,  train score: 0.38514,  validation Loss: 0.01808,  validation score: 0.69794,  current lr:  1.9687440434072263e-05 , Time: 466.45 s\n",
      "Epoch:  35,  train Loss: 0.00084,  train score: 0.38531,  validation Loss: 0.01799,  validation score: 0.69941,  current lr:  1.9687440434072263e-05 , Time: 468.63 s\n",
      "Epoch:  36,  train Loss: 0.00084,  train score: 0.44854,  validation Loss: 0.01846,  validation score: 0.69613,  current lr:  1.9687440434072263e-05 , Time: 469.53 s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.6734e-05.\n",
      "Epoch:  37,  train Loss: 0.00083,  train score: 0.44870,  validation Loss: 0.01861,  validation score: 0.69469,  current lr:  1.673432436896142e-05 , Time: 469.42 s\n",
      "Epoch:  38,  train Loss: 0.00083,  train score: 0.38599,  validation Loss: 0.01884,  validation score: 0.69504,  current lr:  1.673432436896142e-05 , Time: 470.02 s\n",
      "Epoch:  39,  train Loss: 0.00082,  train score: 0.44896,  validation Loss: 0.01881,  validation score: 0.69518,  current lr:  1.673432436896142e-05 , Time: 468.82 s\n",
      "Epoch    40: reducing learning rate of group 0 to 1.4224e-05.\n",
      "Epoch:  40,  train Loss: 0.00082,  train score: 0.44905,  validation Loss: 0.01882,  validation score: 0.69632,  current lr:  1.4224175713617208e-05 , Time: 468.98 s\n",
      "Epoch:  41,  train Loss: 0.00082,  train score: 0.38641,  validation Loss: 0.01930,  validation score: 0.69362,  current lr:  1.4224175713617208e-05 , Time: 470.05 s\n",
      "Epoch:  42,  train Loss: 0.00081,  train score: 0.44469,  validation Loss: 0.01935,  validation score: 0.69307,  current lr:  1.4224175713617208e-05 , Time: 467.66 s\n",
      "Epoch    43: reducing learning rate of group 0 to 1.2091e-05.\n",
      "Epoch:  43,  train Loss: 0.00081,  train score: 0.44476,  validation Loss: 0.01943,  validation score: 0.69279,  current lr:  1.2090549356574626e-05 , Time: 468.99 s\n",
      "Epoch:  44,  train Loss: 0.00081,  train score: 0.38668,  validation Loss: 0.01960,  validation score: 0.69285,  current lr:  1.2090549356574626e-05 , Time: 468.15 s\n",
      "Epoch:  45,  train Loss: 0.00081,  train score: 0.44500,  validation Loss: 0.01967,  validation score: 0.69199,  current lr:  1.2090549356574626e-05 , Time: 469.63 s\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0277e-05.\n",
      "Epoch:  46,  train Loss: 0.00080,  train score: 0.44508,  validation Loss: 0.01972,  validation score: 0.69218,  current lr:  1.0276966953088432e-05 , Time: 469.24 s\n",
      "Epoch:  47,  train Loss: 0.00080,  train score: 0.38699,  validation Loss: 0.01992,  validation score: 0.69197,  current lr:  1.0276966953088432e-05 , Time: 469.97 s\n",
      "Epoch:  48,  train Loss: 0.00080,  train score: 0.38716,  validation Loss: 0.02006,  validation score: 0.69141,  current lr:  1.0276966953088432e-05 , Time: 470.39 s\n",
      "Epoch    49: reducing learning rate of group 0 to 8.7354e-06.\n",
      "Epoch:  49,  train Loss: 0.00080,  train score: 0.44549,  validation Loss: 0.02010,  validation score: 0.69087,  current lr:  8.735421910125167e-06 , Time: 468.86 s\n",
      "Epoch:  50,  train Loss: 0.00079,  train score: 0.45015,  validation Loss: 0.02019,  validation score: 0.69082,  current lr:  8.735421910125167e-06 , Time: 469.62 s\n",
      "Epoch:  51,  train Loss: 0.00079,  train score: 0.45023,  validation Loss: 0.02035,  validation score: 0.69033,  current lr:  8.735421910125167e-06 , Time: 471.33 s\n",
      "Epoch    52: reducing learning rate of group 0 to 7.4251e-06.\n",
      "Epoch:  52,  train Loss: 0.00079,  train score: 0.44559,  validation Loss: 0.02033,  validation score: 0.69062,  current lr:  7.425108623606392e-06 , Time: 470.25 s\n",
      "Epoch:  53,  train Loss: 0.00079,  train score: 0.45041,  validation Loss: 0.02057,  validation score: 0.68971,  current lr:  7.425108623606392e-06 , Time: 470.23 s\n",
      "Epoch:  54,  train Loss: 0.00079,  train score: 0.38760,  validation Loss: 0.02056,  validation score: 0.69033,  current lr:  7.425108623606392e-06 , Time: 469.38 s\n",
      "Epoch    55: reducing learning rate of group 0 to 6.3113e-06.\n",
      "Epoch:  55,  train Loss: 0.00078,  train score: 0.38765,  validation Loss: 0.02068,  validation score: 0.69021,  current lr:  6.3113423300654325e-06 , Time: 469.52 s\n",
      "Epoch:  56,  train Loss: 0.00078,  train score: 0.44587,  validation Loss: 0.02068,  validation score: 0.68985,  current lr:  6.3113423300654325e-06 , Time: 470.13 s\n",
      "Epoch:  57,  train Loss: 0.00078,  train score: 0.45068,  validation Loss: 0.02085,  validation score: 0.68914,  current lr:  6.3113423300654325e-06 , Time: 469.71 s\n",
      "Epoch    58: reducing learning rate of group 0 to 5.3646e-06.\n",
      "Epoch:  58,  train Loss: 0.00078,  train score: 0.45072,  validation Loss: 0.02090,  validation score: 0.68943,  current lr:  5.3646409805556175e-06 , Time: 471.06 s\n",
      "Epoch:  59,  train Loss: 0.00078,  train score: 0.45072,  validation Loss: 0.02104,  validation score: 0.68882,  current lr:  5.3646409805556175e-06 , Time: 470.98 s\n",
      "Epoch:  60,  train Loss: 0.00078,  train score: 0.44611,  validation Loss: 0.02098,  validation score: 0.68917,  current lr:  5.3646409805556175e-06 , Time: 468.61 s\n",
      "Epoch    61: reducing learning rate of group 0 to 4.5599e-06.\n",
      "Epoch:  61,  train Loss: 0.00078,  train score: 0.44613,  validation Loss: 0.02104,  validation score: 0.68903,  current lr:  4.559944833472275e-06 , Time: 469.90 s\n",
      "Epoch:  62,  train Loss: 0.00077,  train score: 0.45100,  validation Loss: 0.02117,  validation score: 0.68883,  current lr:  4.559944833472275e-06 , Time: 471.45 s\n",
      "Epoch:  63,  train Loss: 0.00077,  train score: 0.44624,  validation Loss: 0.02118,  validation score: 0.68865,  current lr:  4.559944833472275e-06 , Time: 470.91 s\n",
      "Epoch    64: reducing learning rate of group 0 to 3.8760e-06.\n",
      "Epoch:  64,  train Loss: 0.00077,  train score: 0.44633,  validation Loss: 0.02121,  validation score: 0.68868,  current lr:  3.875953108451433e-06 , Time: 471.01 s\n",
      "Epoch:  65,  train Loss: 0.00077,  train score: 0.38810,  validation Loss: 0.02133,  validation score: 0.68868,  current lr:  3.875953108451433e-06 , Time: 469.18 s\n",
      "Epoch:  66,  train Loss: 0.00077,  train score: 0.44640,  validation Loss: 0.02133,  validation score: 0.68840,  current lr:  3.875953108451433e-06 , Time: 470.04 s\n",
      "Epoch    67: reducing learning rate of group 0 to 3.2946e-06.\n",
      "Epoch:  67,  train Loss: 0.00077,  train score: 0.38817,  validation Loss: 0.02145,  validation score: 0.68836,  current lr:  3.2945601421837183e-06 , Time: 471.88 s\n",
      "Epoch:  68,  train Loss: 0.00077,  train score: 0.44646,  validation Loss: 0.02143,  validation score: 0.68823,  current lr:  3.2945601421837183e-06 , Time: 470.96 s\n",
      "Epoch:  69,  train Loss: 0.00077,  train score: 0.38823,  validation Loss: 0.02155,  validation score: 0.68817,  current lr:  3.2945601421837183e-06 , Time: 469.06 s\n",
      "Epoch    70: reducing learning rate of group 0 to 2.8004e-06.\n",
      "Epoch:  70,  train Loss: 0.00077,  train score: 0.44651,  validation Loss: 0.02153,  validation score: 0.68799,  current lr:  2.8003761208561607e-06 , Time: 467.24 s\n",
      "Epoch:  71,  train Loss: 0.00076,  train score: 0.44655,  validation Loss: 0.02157,  validation score: 0.68813,  current lr:  2.8003761208561607e-06 , Time: 469.22 s\n",
      "Epoch:  72,  train Loss: 0.00076,  train score: 0.38833,  validation Loss: 0.02169,  validation score: 0.68791,  current lr:  2.8003761208561607e-06 , Time: 470.91 s\n",
      "Epoch    73: reducing learning rate of group 0 to 2.3803e-06.\n",
      "Epoch:  73,  train Loss: 0.00076,  train score: 0.44659,  validation Loss: 0.02168,  validation score: 0.68774,  current lr:  2.3803197027277364e-06 , Time: 469.16 s\n",
      "Epoch:  74,  train Loss: 0.00076,  train score: 0.44663,  validation Loss: 0.02170,  validation score: 0.68785,  current lr:  2.3803197027277364e-06 , Time: 469.78 s\n",
      "Epoch:  75,  train Loss: 0.00076,  train score: 0.45153,  validation Loss: 0.02183,  validation score: 0.68747,  current lr:  2.3803197027277364e-06 , Time: 469.62 s\n",
      "Epoch    76: reducing learning rate of group 0 to 2.0233e-06.\n",
      "Epoch:  76,  train Loss: 0.00076,  train score: 0.38846,  validation Loss: 0.02186,  validation score: 0.68758,  current lr:  2.0232717473185757e-06 , Time: 471.02 s\n",
      "Epoch:  77,  train Loss: 0.00076,  train score: 0.44671,  validation Loss: 0.02183,  validation score: 0.68750,  current lr:  2.0232717473185757e-06 , Time: 469.55 s\n",
      "Epoch:  78,  train Loss: 0.00076,  train score: 0.38849,  validation Loss: 0.02193,  validation score: 0.68742,  current lr:  2.0232717473185757e-06 , Time: 467.83 s\n",
      "Epoch    79: reducing learning rate of group 0 to 1.7198e-06.\n",
      "Epoch:  79,  train Loss: 0.00076,  train score: 0.45160,  validation Loss: 0.02198,  validation score: 0.68712,  current lr:  1.7197809852207893e-06 , Time: 470.85 s\n",
      "Epoch:  80,  train Loss: 0.00076,  train score: 0.45162,  validation Loss: 0.02202,  validation score: 0.68704,  current lr:  1.7197809852207893e-06 , Time: 469.53 s\n",
      "Training Finished after 80 epoches\n"
     ]
    }
   ],
   "source": [
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug_noAffine(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "#     for batch, data in enumerate(trainloader_aug):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'torchio_ol_aug_noAffine_model.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Loss Over Epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss Value')\n",
    "train_curve = plt.plot(history['train_loss'], marker = 'o', label = 'Train loss')\n",
    "validation_curve = plt.plot(history['validation_loss'], marker = 'o', label = 'Validation loss')\n",
    "plt.legend(fontsize = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above result are achieved by training with only augmented data (trained twice, once offline that has affine, once without affine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dice Score 0.8240687175747964\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the unet model at its prime (when it performed the best on the validation set).\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'Basic_Unet_best_model_0428_0.82.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "# Testing process on test data.\n",
    "unet_model.eval()\n",
    "# Getting test data indices for dataloading\n",
    "test_data_indexes = test_indices\n",
    "# Total testing data used.\n",
    "data_length = len(test_data_indexes)\n",
    "# Score after testing on dataset.\n",
    "mean_test_score = 0\n",
    "test_score = []\n",
    "\n",
    "for batch, data in enumerate(testloader):\n",
    "    # Data prepared to be given as input to model.\n",
    "    image = data['image'].to(device, dtype=torch.float)\n",
    "    mask = data['mask']\n",
    "\n",
    "    # Predicted output from the input sample.\n",
    "    mask_prediction = unet_model(image).cpu()\n",
    "    # Threshold elimination.\n",
    "    mask_prediction = (mask_prediction > 0.5)\n",
    "    mask_prediction = mask_prediction.numpy()\n",
    "\n",
    "    mask = np.resize(mask, (155, 240, 240))\n",
    "    mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "\n",
    "    # Calculating the dice score for original and predicted mask.\n",
    "    mean_test_score += dice_coefficient(mask_prediction, mask)\n",
    "    test_score.append(dice_coefficient(mask_prediction, mask))\n",
    "\n",
    "# Calculating the mean score for the whole test dataset.\n",
    "unet_score = mean_test_score / data_length\n",
    "# Putting the model back to training mode.\n",
    "print(f'\\nDice Score {unet_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "score = pd.DataFrame(test_score, columns=['baseline'])\n",
    "score.to_csv('score.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dice Score 0.7722799770250905\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the unet model at its prime (when it performed the best on the validation set).\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'torchio_ol_aug_noAffine_model.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "# Testing process on test data.\n",
    "unet_model.eval()\n",
    "# Getting test data indices for dataloading\n",
    "test_data_indexes = test_indices\n",
    "# Total testing data used.\n",
    "data_length = len(test_data_indexes)\n",
    "# Score after testing on dataset.\n",
    "mean_test_score = 0\n",
    "test_score = []\n",
    "\n",
    "for batch, data in enumerate(testloader):\n",
    "    # Data prepared to be given as input to model.\n",
    "    image = data['image'].to(device, dtype=torch.float)\n",
    "    mask = data['mask']\n",
    "\n",
    "    # Predicted output from the input sample.\n",
    "    mask_prediction = unet_model(image).cpu()\n",
    "    # Threshold elimination.\n",
    "    mask_prediction = (mask_prediction > 0.5)\n",
    "    mask_prediction = mask_prediction.numpy()\n",
    "\n",
    "    mask = np.resize(mask, (155, 240, 240))\n",
    "    mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "\n",
    "    # Calculating the dice score for original and predicted mask.\n",
    "    mean_test_score += dice_coefficient(mask_prediction, mask)\n",
    "    test_score.append(dice_coefficient(mask_prediction, mask))\n",
    "\n",
    "# Calculating the mean score for the whole test dataset.\n",
    "unet_score = mean_test_score / data_length\n",
    "# Putting the model back to training mode.\n",
    "print(f'\\nDice Score {unet_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.read_csv('score.csv')\n",
    "score['torchio_only'] = test_score\n",
    "score.to_csv('score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# May 7th\n",
    "# train with original and ol augmentation without affine. \n",
    "# first we'll train with the last t.7722799 model, then maybe try keep loading the baseline .82 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update: training with last 0.7722799 model is not working, highest is the first, but then it goes down to 0.6\n",
    "# we will retrain the baseline with purely online augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00163,  train score: 0.38119,  validation Loss: 0.01585,  validation score: 0.68918,  current lr:  0.0001 , Time: 909.05 s\tBest model saved at score: 0.68918\n",
      "Epoch:   2,  train Loss: 0.00149,  train score: 0.43070,  validation Loss: 0.01573,  validation score: 0.66324,  current lr:  0.0001 , Time: 905.56 s\n",
      "Epoch:   3,  train Loss: 0.00138,  train score: 0.36418,  validation Loss: 0.01579,  validation score: 0.69734,  current lr:  0.0001 , Time: 903.81 s\tBest model saved at score: 0.69734\n",
      "Epoch:   4,  train Loss: 0.00138,  train score: 0.43414,  validation Loss: 0.01669,  validation score: 0.67204,  current lr:  0.0001 , Time: 904.59 s\n",
      "Epoch:   5,  train Loss: 0.00140,  train score: 0.36563,  validation Loss: 0.01547,  validation score: 0.68595,  current lr:  0.0001 , Time: 902.70 s\n",
      "Epoch:   6,  train Loss: 0.00130,  train score: 0.43426,  validation Loss: 0.01444,  validation score: 0.71504,  current lr:  0.0001 , Time: 904.01 s\tBest model saved at score: 0.71504\n",
      "Epoch:   7,  train Loss: 0.00126,  train score: 0.36978,  validation Loss: 0.01544,  validation score: 0.70918,  current lr:  0.0001 , Time: 907.06 s\n",
      "Epoch:   8,  train Loss: 0.00126,  train score: 0.43775,  validation Loss: 0.01515,  validation score: 0.70234,  current lr:  0.0001 , Time: 911.05 s\n",
      "Epoch     9: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   9,  train Loss: 0.00119,  train score: 0.37266,  validation Loss: 0.01492,  validation score: 0.71051,  current lr:  8.5e-05 , Time: 910.89 s\n",
      "Epoch:  10,  train Loss: 0.00112,  train score: 0.44093,  validation Loss: 0.01438,  validation score: 0.71155,  current lr:  8.5e-05 , Time: 910.26 s\n",
      "Epoch:  11,  train Loss: 0.00106,  train score: 0.37552,  validation Loss: 0.01524,  validation score: 0.72493,  current lr:  8.5e-05 , Time: 910.72 s\tBest model saved at score: 0.72493\n",
      "Epoch:  12,  train Loss: 0.00106,  train score: 0.37593,  validation Loss: 0.01435,  validation score: 0.73263,  current lr:  8.5e-05 , Time: 911.00 s\tBest model saved at score: 0.73263\n",
      "Epoch:  13,  train Loss: 0.00114,  train score: 0.44064,  validation Loss: 0.01423,  validation score: 0.69965,  current lr:  8.5e-05 , Time: 911.32 s\n",
      "Epoch:  14,  train Loss: 0.00108,  train score: 0.37645,  validation Loss: 0.01533,  validation score: 0.71952,  current lr:  8.5e-05 , Time: 910.93 s\n",
      "Epoch:  15,  train Loss: 0.00102,  train score: 0.44449,  validation Loss: 0.01491,  validation score: 0.71012,  current lr:  8.5e-05 , Time: 909.44 s\n",
      "Epoch    16: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:  16,  train Loss: 0.00102,  train score: 0.37840,  validation Loss: 0.01569,  validation score: 0.70838,  current lr:  7.225000000000001e-05 , Time: 910.43 s\n",
      "Epoch:  17,  train Loss: 0.00102,  train score: 0.44401,  validation Loss: 0.01498,  validation score: 0.71668,  current lr:  7.225000000000001e-05 , Time: 910.56 s\n",
      "Epoch:  18,  train Loss: 0.00096,  train score: 0.44864,  validation Loss: 0.01553,  validation score: 0.71570,  current lr:  7.225000000000001e-05 , Time: 910.84 s\n",
      "Epoch    19: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  19,  train Loss: 0.00094,  train score: 0.38045,  validation Loss: 0.01533,  validation score: 0.72190,  current lr:  6.141250000000001e-05 , Time: 909.35 s\n",
      "Epoch:  20,  train Loss: 0.00093,  train score: 0.44768,  validation Loss: 0.01528,  validation score: 0.72464,  current lr:  6.141250000000001e-05 , Time: 911.75 s\n",
      "Training Finished after 20 epoches\n"
     ]
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "# loading the previous best model (from online augmentation)\n",
    "# for the new online augmentation without affine and different probability\n",
    "\n",
    "# a new epoch\n",
    "epochs = 20\n",
    "\n",
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'Basic_Unet_best_model_0428_0.82.pth'))  # the newest .77 model\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)\n",
    "\n",
    "\n",
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "    \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug_noAffine(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "#     for batch, data in enumerate(trainloader_aug):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'ori_torchio_ol_aug_noAffine_model.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The result is good with only 20 epochs, the bst model is 12st epoch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dice Score 0.7974220197205032\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the unet model at its prime (when it performed the best on the validation set).\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'ori_torchio_ol_aug_noAffine_model.pth'))\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "# Testing process on test data.\n",
    "unet_model.eval()\n",
    "# Getting test data indices for dataloading\n",
    "test_data_indexes = test_indices\n",
    "# Total testing data used.\n",
    "data_length = len(test_data_indexes)\n",
    "# Score after testing on dataset.\n",
    "mean_test_score = 0\n",
    "test_score = []\n",
    "\n",
    "for batch, data in enumerate(testloader):\n",
    "    # Data prepared to be given as input to model.\n",
    "    image = data['image'].to(device, dtype=torch.float)\n",
    "    mask = data['mask']\n",
    "\n",
    "    # Predicted output from the input sample.\n",
    "    mask_prediction = unet_model(image).cpu()\n",
    "    # Threshold elimination.\n",
    "    mask_prediction = (mask_prediction > 0.5)\n",
    "    mask_prediction = mask_prediction.numpy()\n",
    "\n",
    "    mask = np.resize(mask, (155, 240, 240))\n",
    "    mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "\n",
    "    # Calculating the dice score for original and predicted mask.\n",
    "    mean_test_score += dice_coefficient(mask_prediction, mask)\n",
    "    test_score.append(dice_coefficient(mask_prediction, mask))\n",
    "\n",
    "# Calculating the mean score for the whole test dataset.\n",
    "unet_score = mean_test_score / data_length\n",
    "# Putting the model back to training mode.\n",
    "print(f'\\nDice Score {unet_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "score = pd.read_csv('score.csv')\n",
    "score['original_olaug_noaffine'] = test_score\n",
    "score.to_csv('score.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# May 7 night update: now the result looks promising after 20 epochs, Keep training this model for 80 epochs. \n",
    "\n",
    "if it doesnot work, we will try loading the baseline 0.82 and train with 80~100 epochs. But theoretically hope it'll work.\n",
    "\n",
    "# midnight update: performs below expectation, will train with baseline model. \n",
    "\n",
    "the original plan above (May 7 night) has a v.score bouncing around 0.7~0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00165,  train score: 0.40070,  validation Loss: 0.01397,  validation score: 0.70575,  current lr:  0.0001 , Time: 916.89 s\tBest model saved at score: 0.70575\n",
      "Epoch:   2,  train Loss: 0.00150,  train score: 0.36487,  validation Loss: 0.01434,  validation score: 0.72741,  current lr:  0.0001 , Time: 913.25 s\tBest model saved at score: 0.72741\n",
      "Epoch:   3,  train Loss: 0.00140,  train score: 0.43148,  validation Loss: 0.01429,  validation score: 0.71677,  current lr:  0.0001 , Time: 913.53 s\n",
      "Epoch     4: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   4,  train Loss: 0.00221,  train score: 0.40165,  validation Loss: 0.02646,  validation score: 0.48835,  current lr:  8.5e-05 , Time: 911.10 s\n",
      "Epoch:   5,  train Loss: 0.00222,  train score: 0.40418,  validation Loss: 0.01574,  validation score: 0.66165,  current lr:  8.5e-05 , Time: 910.93 s\n",
      "Epoch:   6,  train Loss: 0.00132,  train score: 0.36466,  validation Loss: 0.01514,  validation score: 0.68014,  current lr:  8.5e-05 , Time: 910.85 s\n",
      "Epoch     7: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:   7,  train Loss: 0.00116,  train score: 0.43930,  validation Loss: 0.01495,  validation score: 0.69058,  current lr:  7.225000000000001e-05 , Time: 910.11 s\n",
      "Epoch:   8,  train Loss: 0.00110,  train score: 0.37269,  validation Loss: 0.01539,  validation score: 0.69035,  current lr:  7.225000000000001e-05 , Time: 908.52 s\n",
      "Epoch:   9,  train Loss: 0.00105,  train score: 0.37549,  validation Loss: 0.01544,  validation score: 0.69122,  current lr:  7.225000000000001e-05 , Time: 904.65 s\n",
      "Epoch    10: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  10,  train Loss: 0.00104,  train score: 0.37692,  validation Loss: 0.01636,  validation score: 0.68741,  current lr:  6.141250000000001e-05 , Time: 906.77 s\n",
      "Epoch:  11,  train Loss: 0.00104,  train score: 0.44420,  validation Loss: 0.01532,  validation score: 0.68960,  current lr:  6.141250000000001e-05 , Time: 908.58 s\n",
      "Epoch:  12,  train Loss: 0.00104,  train score: 0.44447,  validation Loss: 0.01599,  validation score: 0.68612,  current lr:  6.141250000000001e-05 , Time: 906.69 s\n",
      "Epoch    13: reducing learning rate of group 0 to 5.2201e-05.\n",
      "Epoch:  13,  train Loss: 0.00102,  train score: 0.44474,  validation Loss: 0.01550,  validation score: 0.70025,  current lr:  5.2200625000000005e-05 , Time: 905.26 s\n",
      "Epoch:  14,  train Loss: 0.00101,  train score: 0.37775,  validation Loss: 0.01576,  validation score: 0.70944,  current lr:  5.2200625000000005e-05 , Time: 906.91 s\n",
      "Epoch:  15,  train Loss: 0.00096,  train score: 0.44701,  validation Loss: 0.01680,  validation score: 0.68850,  current lr:  5.2200625000000005e-05 , Time: 907.33 s\n",
      "Epoch    16: reducing learning rate of group 0 to 4.4371e-05.\n",
      "Epoch:  16,  train Loss: 0.00096,  train score: 0.37982,  validation Loss: 0.01655,  validation score: 0.69961,  current lr:  4.437053125e-05 , Time: 906.05 s\n",
      "Epoch:  17,  train Loss: 0.00094,  train score: 0.44827,  validation Loss: 0.01662,  validation score: 0.69672,  current lr:  4.437053125e-05 , Time: 905.75 s\n",
      "Epoch:  18,  train Loss: 0.00095,  train score: 0.44850,  validation Loss: 0.01633,  validation score: 0.70001,  current lr:  4.437053125e-05 , Time: 906.81 s\n",
      "Epoch    19: reducing learning rate of group 0 to 3.7715e-05.\n",
      "Epoch:  19,  train Loss: 0.00092,  train score: 0.37953,  validation Loss: 0.01649,  validation score: 0.70037,  current lr:  3.77149515625e-05 , Time: 908.10 s\n",
      "Epoch:  20,  train Loss: 0.00090,  train score: 0.44950,  validation Loss: 0.01683,  validation score: 0.69758,  current lr:  3.77149515625e-05 , Time: 907.94 s\n",
      "Epoch:  21,  train Loss: 0.00088,  train score: 0.38104,  validation Loss: 0.01706,  validation score: 0.70117,  current lr:  3.77149515625e-05 , Time: 907.03 s\n",
      "Epoch    22: reducing learning rate of group 0 to 3.2058e-05.\n",
      "Epoch:  22,  train Loss: 0.00087,  train score: 0.45111,  validation Loss: 0.01736,  validation score: 0.69219,  current lr:  3.2057708828124995e-05 , Time: 907.33 s\n",
      "Epoch:  23,  train Loss: 0.00087,  train score: 0.38169,  validation Loss: 0.01680,  validation score: 0.70487,  current lr:  3.2057708828124995e-05 , Time: 905.14 s\n",
      "Epoch:  24,  train Loss: 0.00085,  train score: 0.45125,  validation Loss: 0.01746,  validation score: 0.69683,  current lr:  3.2057708828124995e-05 , Time: 906.32 s\n",
      "Epoch    25: reducing learning rate of group 0 to 2.7249e-05.\n",
      "Epoch:  25,  train Loss: 0.00085,  train score: 0.38272,  validation Loss: 0.01759,  validation score: 0.69670,  current lr:  2.7249052503906245e-05 , Time: 908.05 s\n",
      "Epoch:  26,  train Loss: 0.00083,  train score: 0.45194,  validation Loss: 0.01780,  validation score: 0.69815,  current lr:  2.7249052503906245e-05 , Time: 906.47 s\n",
      "Epoch:  27,  train Loss: 0.00082,  train score: 0.38350,  validation Loss: 0.01773,  validation score: 0.69861,  current lr:  2.7249052503906245e-05 , Time: 905.85 s\n",
      "Epoch    28: reducing learning rate of group 0 to 2.3162e-05.\n",
      "Epoch:  28,  train Loss: 0.00081,  train score: 0.45218,  validation Loss: 0.01803,  validation score: 0.69425,  current lr:  2.3161694628320308e-05 , Time: 903.63 s\n",
      "Epoch:  29,  train Loss: 0.00081,  train score: 0.38358,  validation Loss: 0.01831,  validation score: 0.69448,  current lr:  2.3161694628320308e-05 , Time: 902.88 s\n",
      "Epoch:  30,  train Loss: 0.00080,  train score: 0.38429,  validation Loss: 0.01852,  validation score: 0.69582,  current lr:  2.3161694628320308e-05 , Time: 901.92 s\n",
      "Epoch    31: reducing learning rate of group 0 to 1.9687e-05.\n",
      "Epoch:  31,  train Loss: 0.00081,  train score: 0.45235,  validation Loss: 0.01805,  validation score: 0.70394,  current lr:  1.9687440434072263e-05 , Time: 901.88 s\n",
      "Epoch:  32,  train Loss: 0.00081,  train score: 0.38446,  validation Loss: 0.01862,  validation score: 0.69244,  current lr:  1.9687440434072263e-05 , Time: 900.77 s\n",
      "Epoch:  33,  train Loss: 0.00078,  train score: 0.45312,  validation Loss: 0.01888,  validation score: 0.69491,  current lr:  1.9687440434072263e-05 , Time: 901.30 s\n",
      "Epoch    34: reducing learning rate of group 0 to 1.6734e-05.\n",
      "Epoch:  34,  train Loss: 0.00078,  train score: 0.38535,  validation Loss: 0.01929,  validation score: 0.69313,  current lr:  1.673432436896142e-05 , Time: 901.40 s\n",
      "Epoch:  35,  train Loss: 0.00077,  train score: 0.45388,  validation Loss: 0.01915,  validation score: 0.69412,  current lr:  1.673432436896142e-05 , Time: 902.07 s\n",
      "Epoch:  36,  train Loss: 0.00076,  train score: 0.45372,  validation Loss: 0.01943,  validation score: 0.69326,  current lr:  1.673432436896142e-05 , Time: 905.80 s\n",
      "Epoch    37: reducing learning rate of group 0 to 1.4224e-05.\n",
      "Epoch:  37,  train Loss: 0.00076,  train score: 0.38570,  validation Loss: 0.01999,  validation score: 0.68957,  current lr:  1.4224175713617208e-05 , Time: 907.41 s\n",
      "Epoch:  38,  train Loss: 0.00076,  train score: 0.45419,  validation Loss: 0.01964,  validation score: 0.69340,  current lr:  1.4224175713617208e-05 , Time: 910.18 s\n",
      "Epoch:  39,  train Loss: 0.00075,  train score: 0.45461,  validation Loss: 0.01991,  validation score: 0.69222,  current lr:  1.4224175713617208e-05 , Time: 909.39 s\n",
      "Epoch    40: reducing learning rate of group 0 to 1.2091e-05.\n",
      "Epoch:  40,  train Loss: 0.00075,  train score: 0.38595,  validation Loss: 0.02010,  validation score: 0.68953,  current lr:  1.2090549356574626e-05 , Time: 907.34 s\n",
      "Epoch:  41,  train Loss: 0.00074,  train score: 0.38603,  validation Loss: 0.02031,  validation score: 0.69083,  current lr:  1.2090549356574626e-05 , Time: 907.41 s\n",
      "Epoch:  42,  train Loss: 0.00074,  train score: 0.38624,  validation Loss: 0.02043,  validation score: 0.69070,  current lr:  1.2090549356574626e-05 , Time: 909.33 s\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0277e-05.\n",
      "Epoch:  43,  train Loss: 0.00074,  train score: 0.45535,  validation Loss: 0.02018,  validation score: 0.69149,  current lr:  1.0276966953088432e-05 , Time: 908.31 s\n",
      "Epoch:  44,  train Loss: 0.00073,  train score: 0.45536,  validation Loss: 0.02063,  validation score: 0.69064,  current lr:  1.0276966953088432e-05 , Time: 910.28 s\n",
      "Epoch:  45,  train Loss: 0.00073,  train score: 0.45531,  validation Loss: 0.02098,  validation score: 0.68798,  current lr:  1.0276966953088432e-05 , Time: 905.47 s\n",
      "Epoch    46: reducing learning rate of group 0 to 8.7354e-06.\n",
      "Epoch:  46,  train Loss: 0.00073,  train score: 0.38674,  validation Loss: 0.02082,  validation score: 0.69117,  current lr:  8.735421910125167e-06 , Time: 908.91 s\n",
      "Epoch:  47,  train Loss: 0.00072,  train score: 0.38698,  validation Loss: 0.02112,  validation score: 0.68991,  current lr:  8.735421910125167e-06 , Time: 908.91 s\n",
      "Epoch:  48,  train Loss: 0.00072,  train score: 0.45584,  validation Loss: 0.02108,  validation score: 0.68927,  current lr:  8.735421910125167e-06 , Time: 909.50 s\n",
      "Epoch    49: reducing learning rate of group 0 to 7.4251e-06.\n",
      "Epoch:  49,  train Loss: 0.00072,  train score: 0.45592,  validation Loss: 0.02135,  validation score: 0.68911,  current lr:  7.425108623606392e-06 , Time: 910.11 s\n",
      "Epoch:  50,  train Loss: 0.00071,  train score: 0.45612,  validation Loss: 0.02152,  validation score: 0.68853,  current lr:  7.425108623606392e-06 , Time: 909.41 s\n",
      "Epoch:  51,  train Loss: 0.00071,  train score: 0.45618,  validation Loss: 0.02161,  validation score: 0.68783,  current lr:  7.425108623606392e-06 , Time: 908.26 s\n",
      "Epoch    52: reducing learning rate of group 0 to 6.3113e-06.\n",
      "Epoch:  52,  train Loss: 0.00071,  train score: 0.38738,  validation Loss: 0.02142,  validation score: 0.68957,  current lr:  6.3113423300654325e-06 , Time: 908.19 s\n",
      "Epoch:  53,  train Loss: 0.00071,  train score: 0.45606,  validation Loss: 0.02201,  validation score: 0.68683,  current lr:  6.3113423300654325e-06 , Time: 911.08 s\n",
      "Epoch:  54,  train Loss: 0.00071,  train score: 0.38754,  validation Loss: 0.02190,  validation score: 0.68723,  current lr:  6.3113423300654325e-06 , Time: 907.75 s\n",
      "Epoch    55: reducing learning rate of group 0 to 5.3646e-06.\n",
      "Epoch:  55,  train Loss: 0.00070,  train score: 0.38751,  validation Loss: 0.02197,  validation score: 0.68731,  current lr:  5.3646409805556175e-06 , Time: 909.55 s\n",
      "Epoch:  56,  train Loss: 0.00070,  train score: 0.38759,  validation Loss: 0.02218,  validation score: 0.68629,  current lr:  5.3646409805556175e-06 , Time: 909.92 s\n",
      "Epoch:  57,  train Loss: 0.00070,  train score: 0.38771,  validation Loss: 0.02221,  validation score: 0.68668,  current lr:  5.3646409805556175e-06 , Time: 909.10 s\n",
      "Epoch    58: reducing learning rate of group 0 to 4.5599e-06.\n",
      "Epoch:  58,  train Loss: 0.00070,  train score: 0.45667,  validation Loss: 0.02229,  validation score: 0.68587,  current lr:  4.559944833472275e-06 , Time: 908.00 s\n",
      "Epoch:  59,  train Loss: 0.00070,  train score: 0.45640,  validation Loss: 0.02253,  validation score: 0.68547,  current lr:  4.559944833472275e-06 , Time: 908.12 s\n",
      "Epoch:  60,  train Loss: 0.00069,  train score: 0.45644,  validation Loss: 0.02263,  validation score: 0.68480,  current lr:  4.559944833472275e-06 , Time: 910.31 s\n",
      "Epoch    61: reducing learning rate of group 0 to 3.8760e-06.\n",
      "Epoch:  61,  train Loss: 0.00069,  train score: 0.38796,  validation Loss: 0.02274,  validation score: 0.68486,  current lr:  3.875953108451433e-06 , Time: 908.96 s\n",
      "Epoch:  62,  train Loss: 0.00069,  train score: 0.38800,  validation Loss: 0.02278,  validation score: 0.68482,  current lr:  3.875953108451433e-06 , Time: 908.53 s\n",
      "Epoch:  63,  train Loss: 0.00069,  train score: 0.45670,  validation Loss: 0.02289,  validation score: 0.68438,  current lr:  3.875953108451433e-06 , Time: 908.75 s\n",
      "Epoch    64: reducing learning rate of group 0 to 3.2946e-06.\n",
      "Epoch:  64,  train Loss: 0.00069,  train score: 0.45711,  validation Loss: 0.02305,  validation score: 0.68384,  current lr:  3.2945601421837183e-06 , Time: 907.23 s\n",
      "Epoch:  65,  train Loss: 0.00069,  train score: 0.45715,  validation Loss: 0.02315,  validation score: 0.68376,  current lr:  3.2945601421837183e-06 , Time: 909.40 s\n",
      "Epoch:  66,  train Loss: 0.00068,  train score: 0.45719,  validation Loss: 0.02321,  validation score: 0.68362,  current lr:  3.2945601421837183e-06 , Time: 907.11 s\n",
      "Epoch    67: reducing learning rate of group 0 to 2.8004e-06.\n",
      "Epoch:  67,  train Loss: 0.00068,  train score: 0.45726,  validation Loss: 0.02329,  validation score: 0.68339,  current lr:  2.8003761208561607e-06 , Time: 909.55 s\n",
      "Epoch:  68,  train Loss: 0.00068,  train score: 0.45730,  validation Loss: 0.02339,  validation score: 0.68330,  current lr:  2.8003761208561607e-06 , Time: 908.39 s\n",
      "Epoch:  69,  train Loss: 0.00068,  train score: 0.45693,  validation Loss: 0.02343,  validation score: 0.68320,  current lr:  2.8003761208561607e-06 , Time: 906.62 s\n",
      "Epoch    70: reducing learning rate of group 0 to 2.3803e-06.\n",
      "Epoch:  70,  train Loss: 0.00068,  train score: 0.45735,  validation Loss: 0.02356,  validation score: 0.68275,  current lr:  2.3803197027277364e-06 , Time: 910.56 s\n",
      "Epoch:  71,  train Loss: 0.00068,  train score: 0.45702,  validation Loss: 0.02362,  validation score: 0.68265,  current lr:  2.3803197027277364e-06 , Time: 911.09 s\n",
      "Epoch:  72,  train Loss: 0.00068,  train score: 0.45746,  validation Loss: 0.02374,  validation score: 0.68217,  current lr:  2.3803197027277364e-06 , Time: 909.98 s\n",
      "Epoch    73: reducing learning rate of group 0 to 2.0233e-06.\n",
      "Epoch:  73,  train Loss: 0.00068,  train score: 0.45709,  validation Loss: 0.02378,  validation score: 0.68217,  current lr:  2.0232717473185757e-06 , Time: 909.97 s\n",
      "Epoch:  74,  train Loss: 0.00068,  train score: 0.38850,  validation Loss: 0.02390,  validation score: 0.68174,  current lr:  2.0232717473185757e-06 , Time: 908.95 s\n",
      "Epoch:  75,  train Loss: 0.00067,  train score: 0.45714,  validation Loss: 0.02397,  validation score: 0.68138,  current lr:  2.0232717473185757e-06 , Time: 909.85 s\n",
      "Epoch    76: reducing learning rate of group 0 to 1.7198e-06.\n",
      "Epoch:  76,  train Loss: 0.00067,  train score: 0.45763,  validation Loss: 0.02407,  validation score: 0.68122,  current lr:  1.7197809852207893e-06 , Time: 909.71 s\n",
      "Epoch:  77,  train Loss: 0.00067,  train score: 0.45723,  validation Loss: 0.02409,  validation score: 0.68123,  current lr:  1.7197809852207893e-06 , Time: 909.21 s\n",
      "Epoch:  78,  train Loss: 0.00067,  train score: 0.45725,  validation Loss: 0.02421,  validation score: 0.68074,  current lr:  1.7197809852207893e-06 , Time: 909.22 s\n",
      "Epoch    79: reducing learning rate of group 0 to 1.4618e-06.\n",
      "Epoch:  79,  train Loss: 0.00067,  train score: 0.45774,  validation Loss: 0.02429,  validation score: 0.68068,  current lr:  1.4618138374376709e-06 , Time: 908.08 s\n",
      "Epoch:  80,  train Loss: 0.00067,  train score: 0.45775,  validation Loss: 0.02433,  validation score: 0.68068,  current lr:  1.4618138374376709e-06 , Time: 906.85 s\n",
      "Training Finished after 80 epoches\n"
     ]
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "# loading the previous best model (from online augmentation)\n",
    "# for the new online augmentation without affine and different probability\n",
    "\n",
    "# a new epoch\n",
    "epochs = 80\n",
    "\n",
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'Basic_Unet_best_model_0428_0.82.pth'))  # the newest v.73 model\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)\n",
    "\n",
    "\n",
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "    \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug_noAffine(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "#     for batch, data in enumerate(trainloader_aug):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'ori_torchio_ol_aug_noAffine_model_80.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.00120,  train score: 0.37098,  validation Loss: 0.01915,  validation score: 0.64900,  current lr:  0.0001 , Time: 889.14 s\tBest model saved at score: 0.64900\n",
      "Epoch:   2,  train Loss: 0.00116,  train score: 0.44199,  validation Loss: 0.01900,  validation score: 0.65674,  current lr:  0.0001 , Time: 874.37 s\tBest model saved at score: 0.65674\n",
      "Epoch:   3,  train Loss: 0.00120,  train score: 0.43934,  validation Loss: 0.01552,  validation score: 0.68771,  current lr:  0.0001 , Time: 874.91 s\tBest model saved at score: 0.68771\n",
      "Epoch:   4,  train Loss: 0.00120,  train score: 0.44228,  validation Loss: 0.01714,  validation score: 0.67434,  current lr:  0.0001 , Time: 874.96 s\n",
      "Epoch:   5,  train Loss: 0.00120,  train score: 0.37356,  validation Loss: 0.02348,  validation score: 0.57818,  current lr:  0.0001 , Time: 874.30 s\n",
      "Epoch     6: reducing learning rate of group 0 to 8.5000e-05.\n",
      "Epoch:   6,  train Loss: 0.00385,  train score: 0.37552,  validation Loss: 0.02603,  validation score: 0.46385,  current lr:  8.5e-05 , Time: 873.68 s\n",
      "Epoch:   7,  train Loss: 0.00223,  train score: 0.34135,  validation Loss: 0.02034,  validation score: 0.57948,  current lr:  8.5e-05 , Time: 874.56 s\n",
      "Epoch:   8,  train Loss: 0.00121,  train score: 0.37039,  validation Loss: 0.01988,  validation score: 0.59328,  current lr:  8.5e-05 , Time: 874.11 s\n",
      "Epoch     9: reducing learning rate of group 0 to 7.2250e-05.\n",
      "Epoch:   9,  train Loss: 0.00103,  train score: 0.44400,  validation Loss: 0.02061,  validation score: 0.60224,  current lr:  7.225000000000001e-05 , Time: 872.96 s\n",
      "Epoch:  10,  train Loss: 0.00097,  train score: 0.44723,  validation Loss: 0.02166,  validation score: 0.60166,  current lr:  7.225000000000001e-05 , Time: 874.52 s\n",
      "Epoch:  11,  train Loss: 0.00092,  train score: 0.38013,  validation Loss: 0.02244,  validation score: 0.60239,  current lr:  7.225000000000001e-05 , Time: 874.79 s\n",
      "Epoch    12: reducing learning rate of group 0 to 6.1413e-05.\n",
      "Epoch:  12,  train Loss: 0.00091,  train score: 0.44902,  validation Loss: 0.02319,  validation score: 0.59404,  current lr:  6.141250000000001e-05 , Time: 872.95 s\n",
      "Epoch:  13,  train Loss: 0.00091,  train score: 0.38140,  validation Loss: 0.02317,  validation score: 0.60015,  current lr:  6.141250000000001e-05 , Time: 874.82 s\n",
      "Epoch:  14,  train Loss: 0.00089,  train score: 0.38189,  validation Loss: 0.02322,  validation score: 0.59709,  current lr:  6.141250000000001e-05 , Time: 873.79 s\n",
      "Epoch    15: reducing learning rate of group 0 to 5.2201e-05.\n",
      "Epoch:  15,  train Loss: 0.00089,  train score: 0.45053,  validation Loss: 0.02282,  validation score: 0.60009,  current lr:  5.2200625000000005e-05 , Time: 874.29 s\n",
      "Epoch:  16,  train Loss: 0.00086,  train score: 0.45053,  validation Loss: 0.02361,  validation score: 0.59978,  current lr:  5.2200625000000005e-05 , Time: 872.61 s\n",
      "Epoch:  17,  train Loss: 0.00085,  train score: 0.45151,  validation Loss: 0.02335,  validation score: 0.60680,  current lr:  5.2200625000000005e-05 , Time: 874.03 s\n",
      "Epoch    18: reducing learning rate of group 0 to 4.4371e-05.\n",
      "Epoch:  18,  train Loss: 0.00083,  train score: 0.45160,  validation Loss: 0.02400,  validation score: 0.60619,  current lr:  4.437053125e-05 , Time: 875.39 s\n",
      "Epoch:  19,  train Loss: 0.00082,  train score: 0.45183,  validation Loss: 0.02389,  validation score: 0.60586,  current lr:  4.437053125e-05 , Time: 873.52 s\n",
      "Epoch:  20,  train Loss: 0.00081,  train score: 0.45273,  validation Loss: 0.02434,  validation score: 0.60547,  current lr:  4.437053125e-05 , Time: 875.51 s\n",
      "Epoch    21: reducing learning rate of group 0 to 3.7715e-05.\n",
      "Epoch:  21,  train Loss: 0.00080,  train score: 0.45264,  validation Loss: 0.02488,  validation score: 0.61023,  current lr:  3.77149515625e-05 , Time: 873.71 s\n",
      "Epoch:  22,  train Loss: 0.00080,  train score: 0.45306,  validation Loss: 0.02497,  validation score: 0.60990,  current lr:  3.77149515625e-05 , Time: 875.57 s\n",
      "Epoch:  23,  train Loss: 0.00080,  train score: 0.38497,  validation Loss: 0.02496,  validation score: 0.60315,  current lr:  3.77149515625e-05 , Time: 874.49 s\n",
      "Epoch    24: reducing learning rate of group 0 to 3.2058e-05.\n",
      "Epoch:  24,  train Loss: 0.00079,  train score: 0.45368,  validation Loss: 0.02541,  validation score: 0.60449,  current lr:  3.2057708828124995e-05 , Time: 875.59 s\n",
      "Epoch:  25,  train Loss: 0.00077,  train score: 0.45389,  validation Loss: 0.02542,  validation score: 0.60982,  current lr:  3.2057708828124995e-05 , Time: 872.50 s\n",
      "Epoch:  26,  train Loss: 0.00076,  train score: 0.38562,  validation Loss: 0.02600,  validation score: 0.60638,  current lr:  3.2057708828124995e-05 , Time: 872.06 s\n",
      "Epoch    27: reducing learning rate of group 0 to 2.7249e-05.\n",
      "Epoch:  27,  train Loss: 0.00076,  train score: 0.45500,  validation Loss: 0.02654,  validation score: 0.60687,  current lr:  2.7249052503906245e-05 , Time: 872.33 s\n",
      "Epoch:  28,  train Loss: 0.00075,  train score: 0.45448,  validation Loss: 0.02586,  validation score: 0.61040,  current lr:  2.7249052503906245e-05 , Time: 873.54 s\n",
      "Epoch:  29,  train Loss: 0.00075,  train score: 0.45429,  validation Loss: 0.02660,  validation score: 0.60387,  current lr:  2.7249052503906245e-05 , Time: 873.19 s\n",
      "Epoch    30: reducing learning rate of group 0 to 2.3162e-05.\n",
      "Epoch:  30,  train Loss: 0.00074,  train score: 0.45491,  validation Loss: 0.02660,  validation score: 0.60849,  current lr:  2.3161694628320308e-05 , Time: 873.83 s\n",
      "Training Finished after 30 epoches\n"
     ]
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "# loading the previous best model (from online augmentation)\n",
    "# for the new online augmentation without affine and different probability\n",
    "\n",
    "# a new epoch\n",
    "epochs = 30\n",
    "\n",
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './baseline&torchio_saved_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)\n",
    "# Use when you want to continue training a pre-train model.\n",
    "state_dict = torch.load(os.path.join(model_save_path, 'ori_torchio_ol_aug_noAffine_model_80.pth'))  # the newest v.73 model\n",
    "unet_model.load_state_dict(state_dict)\n",
    "\n",
    "#### You can uncomment this to see the textual architecture of our U-Net.\n",
    "#print(unet_model)\n",
    "\n",
    "\n",
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "    \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug_noAffine(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "#     for batch, data in enumerate(trainloader_aug):\n",
    "#         # Keeping track how many iteration is happening.\n",
    "#         batch_iteration += 1\n",
    "#         # Loading data to device used.\n",
    "#         image = data['image'].to(device, dtype=torch.float)\n",
    "#         mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "#         #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "#         # Clearing gradients of optimizer.\n",
    "#         optimizer.zero_grad()\n",
    "#         # Calculation predicted output using forward pass.\n",
    "#         output = unet_model(image)\n",
    "#         # Calculating the loss value.\n",
    "#         loss_value = criterion(output, mask)\n",
    "#         # Computing the gradients.\n",
    "#         loss_value.backward()\n",
    "#         # Optimizing the network parameters.\n",
    "#         optimizer.step()\n",
    "#         # Updating the running training loss\n",
    "#         train_epoch_loss += loss_value.item()\n",
    "#         train_batch_loss += loss_value.item()\n",
    "        \n",
    "#         mask_prediction = unet_model(image)\n",
    "#         mask_prediction = (mask_prediction > 0.5)\n",
    "#         mask_prediction = mask_prediction.cpu().numpy()\n",
    "#         mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "#         mask = mask.cpu().numpy()\n",
    "#         # Calculate the dice score for original and predicted image mask.\n",
    "#         train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "#         # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "#         if mini_batch:\n",
    "#             if (batch + 1) % mini_batch == 0:\n",
    "#                 train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "#                 print(\n",
    "#                     f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "#                 train_batch_loss = 0\n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'ori_torchio_ol_aug_noAffine_model_80.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def result(image, mask, output, title, transparency = 0.38, save_path = None):\n",
    "    '''\n",
    "    Plots a 2x3 plot with comparisons of output and original image.\n",
    "    Works best with Jupyter Notebook/Lab.\n",
    "    Parameters:\n",
    "        image(numpy.ndarray): Array containing the original image of MRI scan.\n",
    "        mask(numpy.ndarray): Array containing the original mask of tumor.\n",
    "        output(numpy.ndarray): Model predicted mask from input image.\n",
    "        title(str): Title of the plot to be used.\n",
    "        transparency(float): Transparency level of mask on images.\n",
    "                             Default: 0.38\n",
    "        save_path(str): Saves the plot to the location specified.\n",
    "                        Does nothing if None. \n",
    "                        Default: None\n",
    "    Return:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(\n",
    "        10, 8), gridspec_kw={'wspace': 0.025, 'hspace': 0.010})\n",
    "    fig.suptitle(title, x=0.5, y=0.92, fontsize=20)\n",
    "\n",
    "    axs[0][0].set_title(\"Original Mask\", fontdict={'fontsize': 16})\n",
    "    axs[0][0].imshow(mask, cmap='gray')\n",
    "    axs[0][0].set_axis_off()\n",
    "\n",
    "    axs[0][1].set_title(\"Predicted Mask\", fontdict={'fontsize': 16})\n",
    "    axs[0][1].imshow(output, cmap='gray')\n",
    "    axs[0][1].set_axis_off()\n",
    "\n",
    "    mask_diff = np.abs(np.subtract(mask, output))\n",
    "    axs[0][2].set_title(\"Mask Difference\", fontdict={'fontsize': 16})\n",
    "    axs[0][2].imshow(mask_diff, cmap='gray')\n",
    "    axs[0][2].set_axis_off()\n",
    "\n",
    "    seg_output = mask*transparency\n",
    "    seg_image = np.add(image, seg_output)/2\n",
    "    axs[1][0].set_title(\"Original Segmentation\", fontdict={'fontsize': 16})\n",
    "    axs[1][0].imshow(seg_image, cmap='gray')\n",
    "    axs[1][0].set_axis_off()\n",
    "    \n",
    "    seg_output = output*transparency\n",
    "    seg_image = np.add(image, seg_output)/2\n",
    "    axs[1][1].set_title(\"Predicted Segmentation\", fontdict={'fontsize': 16})\n",
    "    axs[1][1].imshow(seg_image, cmap='gray')\n",
    "    axs[1][1].set_axis_off()\n",
    "\n",
    "    axs[1][2].set_title(\"Original Input Image\", fontdict={'fontsize': 16})\n",
    "    axs[1][2].imshow(image, cmap='gray')\n",
    "    axs[1][2].set_axis_off()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi = 90, bbox_inches = 'tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f8be799249fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'Name: {image_index}.png   Dice Score: {d_score:.5f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# save_path = os.path.join('images',f'{d_score:.5f}_{image_index}.png')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-02464a1852d8>\u001b[0m in \u001b[0;36mresult\u001b[0;34m(image, mask, output, title, transparency, save_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis_off\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mmask_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubtract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mask Difference\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'fontsize'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_diff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: numpy boolean subtract, the `-` operator, is not supported, use the bitwise_xor, the `^` operator, or the logical_xor function instead."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for example_index in range(10):\n",
    "    # The purpose of image_index is to make sure we truly pick from the test set.\n",
    "    image_index = test_indices[example_index]\n",
    "    sample = tumor_dataset[image_index]\n",
    "    threshold = 0.5\n",
    "\n",
    "    unet_model.eval()\n",
    "    image = sample['image'].numpy()\n",
    "    mask = sample['mask'].numpy()\n",
    "\n",
    "    image_tensor = torch.Tensor(image)\n",
    "    image_tensor = image_tensor.view((-1, 155, 240, 240)).to(device)\n",
    "    output = unet_model(image_tensor).detach().cpu()\n",
    "    output = (output > threshold)\n",
    "    output = output.numpy()\n",
    "    d_score = dice_coefficient(output, mask)\n",
    "\n",
    "    image = np.resize(image, (155, 240, 240))\n",
    "    image = image[70,:,:]\n",
    "    mask = np.resize(mask, (155, 240, 240))\n",
    "    mask = mask[70,:,:]\n",
    "    output = np.resize(output, (155, 240, 240))\n",
    "    output = output[70,:,:]\n",
    "    # score(float): Sørensen–Dice Coefficient for mask and output. Calculates how similar are the two images.\n",
    "    #d_score = dice_coefficient(output, mask)\n",
    "\n",
    "    title = f'Name: {image_index}.png   Dice Score: {d_score:.5f}'\n",
    "    # save_path = os.path.join('images',f'{d_score:.5f}_{image_index}.png')\n",
    "    result(image, mask, output, title, save_path = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 BMEN4460",
   "language": "python",
   "name": "bmen4460"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
