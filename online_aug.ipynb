{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is used for online augmentation; will us dataLoader, ol_aug_noAffine(), and save_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Link: https://arxiv.org/abs/2003.04696\n",
      "\n",
      "TorchIO version: 0.15.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "import enum\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import tempfile\n",
    "import subprocess\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import pandas as pd\n",
    "\n",
    "from unet import U_Net\n",
    "\n",
    "from glob import glob\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torchio\n",
    "from torchio import AFFINE, DATA, PATH, TYPE, STEM\n",
    "\n",
    "from torch.utils.data import Dataset, SubsetRandomSampler\n",
    "\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy import stats\n",
    "import SimpleITK as sitk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from tqdm import tqdm\n",
    "\n",
    "print('TorchIO version:', torchio.__version__)\n",
    "\n",
    "training_split_ratio = 0.9\n",
    "num_epochs = 5\n",
    "compute_histograms = False\n",
    "train_whole_images = False\n",
    "train_patches = False\n",
    "\n",
    "from torchio.transforms import (\n",
    "    RandomFlip, # check axis\n",
    "    RandomAffine,\n",
    "    RandomElasticDeformation,\n",
    "    RandomNoise, # lower noise\n",
    "    RandomMotion, # no use\n",
    "    RandomBiasField,\n",
    "    RandomSpike,\n",
    "    RandomBlur,\n",
    "    Rescale,\n",
    "    Resample,\n",
    "    ToCanonical,\n",
    "    ZNormalization,\n",
    "    CenterCropOrPad,\n",
    "    HistogramStandardization,\n",
    "    Interpolation,\n",
    "    RandomGhosting\n",
    ")\n",
    "\n",
    "seed = 4460\n",
    "torch.manual_seed(4460)\n",
    "np.random.seed(4460)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation Details\n",
      "\tDevice Used: (cuda)  Tesla K80\n",
      "\n",
      "Packages Used Versions:-\n",
      "\tPytorch Version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print('Computation Details')\n",
    "print(f'\\tDevice Used: ({device})  {torch.cuda.get_device_name(torch.cuda.current_device())}\\n')\n",
    "\n",
    "print('Packages Used Versions:-')\n",
    "print(f'\\tPytorch Version: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prepare data/dataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset folder used\n",
    "DATASET_PATH = os.path.join('./data')\n",
    "\n",
    "# We would like to perform a train-validation-test split at the ratio of T:V:T = 8:1:1.\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Batch size for training. Limited by GPU memory\n",
    "BATCH_SIZE = 6\n",
    "# Training Epochs\n",
    "epochs = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TumorDataset(Dataset):\n",
    "    '''\n",
    "    Returns a TumorDataset class object which represents our tumor dataset.\n",
    "    TumorDataset inherits from torch.utils.data.Dataset class.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, root_dir, DEBUG = False):\n",
    "        '''\n",
    "        Constructor for our TumorDataset class.\n",
    "        Parameters:\n",
    "            root_dir(str): Directory with all the images.\n",
    "            DEBUG(bool): To switch to debug mode for image transformation.\n",
    "\n",
    "        Returns: None\n",
    "        '''\n",
    "        self.root_dir = root_dir\n",
    "        # The default transformation is composed of \n",
    "        # 1) a grayscale conversion.\n",
    "        self.default_transformation = transforms.Compose([\n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        self.DEBUG = DEBUG\n",
    "            \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Overridden method from inheritted class to support\n",
    "        indexing of dataset such that datset[I] can be used\n",
    "        to get Ith sample.\n",
    "        Parameters:\n",
    "            index(int): Index of the dataset sample\n",
    "            \n",
    "        Return:\n",
    "            sample(dict): Contains the index, image, mask torch.Tensor.\n",
    "                        'index': Index of the image.\n",
    "                        'image': Contains the tumor image torch.Tensor.\n",
    "                        'mask' : Contains the mask image torch.Tensor.\n",
    "        '''\n",
    "        # Find the filenames for the tumor images and masks.\n",
    "        image_path = os.path.join(self.root_dir, \"all_gbm_pre_reg\")\n",
    "        tumor_path = os.path.join(self.root_dir, \"all_tumors_reg\")\n",
    "        \n",
    "        image_name = sorted(glob(os.path.join(image_path, \"*t1reg.nii.gz\")))[index]\n",
    "        mask_name = sorted(glob(os.path.join(tumor_path, \"*seg_reg.nii.gz\")))[index]\n",
    "\n",
    "        # Use nibabel to open the images and masks.\n",
    "        image = nib.load(image_name).get_fdata()\n",
    "        mask = nib.load(mask_name).get_fdata()\n",
    "        mask = (mask > 0)\n",
    "\n",
    "        # Apply the default transformations on the images and masks.\n",
    "        #image = self.default_transformation(image)\n",
    "        #mask = self.default_transformation(mask)\n",
    "\n",
    "        # Convert the images and masks to tensor.\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        \n",
    "        # Construct the images and masks together in the form of a dictionary.\n",
    "        sample = {'index': index, 'image': image, 'mask': mask}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        '''Overridden method from inheritted class so that\n",
    "        len(self) returns the size of the dataset.\n",
    "        '''\n",
    "        error_msg = 'Part of dataset is missing!\\nNumber of tumor and mask images are not same.'\n",
    "        total_image_files = len(glob(os.path.join(self.root_dir, 'all_gbm_pre_reg', '*t1reg.nii.gz')))\n",
    "        total_tumor_files = len(glob(os.path.join(self.root_dir, 'all_tumors_reg', '*seg_reg.nii.gz')))\n",
    "\n",
    "        # Sanity check: the number of files shall be even since tumor images and masks are in pairs.\n",
    "        assert total_image_files == total_tumor_files, error_msg\n",
    "        \n",
    "        # Return how many image-mask pairs we have.\n",
    "        return total_image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indices(length, val_split, test_split):\n",
    "    '''\n",
    "    Gets the Training & Testing data indices for the dataset.\n",
    "    Stores the indices and returns them back when the same dataset is used.\n",
    "    Inputs:\n",
    "        length(int): Length of the dataset used.\n",
    "        val_split: the portion (0 to 1) of data used for validation.\n",
    "        test_split: the portion (0 to 1) of data used for testing.\n",
    "    Parameters:\n",
    "        train_indices(list): Array of indices used for training purpose.\n",
    "        validation_indices(list): Array of indices used for validation purpose.\n",
    "        test_indices(list): Array of indices used for testing purpose.\n",
    "    '''\n",
    "    data = dict()\n",
    "    indices = list(range(length))\n",
    "    np.random.shuffle(indices)\n",
    "    split1 = int(np.floor(test_split * len(tumor_dataset)))\n",
    "    split2 = split1 + int(np.floor(val_split * len(tumor_dataset)))\n",
    "    train_indices, validation_indices, test_indices = indices[split2:], indices[split1:split2], indices[:split1]\n",
    "    return train_indices, validation_indices, test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(predicted, target):\n",
    "    '''\n",
    "    Calculates the Sørensen–Dice Coefficient for a single sample.\n",
    "    Parameters:\n",
    "        predicted(numpy.ndarray): Predicted single output of the network.\n",
    "                                Shape - (Channel, Height, Width)\n",
    "        target(numpy.ndarray): Actual required single output for the network\n",
    "                                Shape - (Channel, Height, Width)\n",
    "\n",
    "    Returns:\n",
    "        coefficient(float): Dice coefficient for the input sample.\n",
    "                                    1 represents highest similarity and\n",
    "                                    0 represents lowest similarity.\n",
    "    '''\n",
    "    # The smooth term is used to prevent division by zero.\n",
    "    smooth = 1\n",
    "    product = np.multiply(predicted, target)\n",
    "    intersection = np.sum(product)\n",
    "    coefficient = (2 * intersection + smooth) / (np.sum(predicted) + np.sum(target) + smooth)\n",
    "    return coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the train set: 269 \n",
      "Number of files in the validation set: 33 \n",
      "Number of files in the test set: 33\n"
     ]
    }
   ],
   "source": [
    "tumor_dataset = TumorDataset(DATASET_PATH)\n",
    "\n",
    "\n",
    "train_indices, validation_indices, test_indices = get_indices(len(tumor_dataset), val_split = VAL_SPLIT, test_split = TEST_SPLIT)\n",
    "train_sampler, validation_sampler, test_sampler = SubsetRandomSampler(train_indices), SubsetRandomSampler(validation_indices), SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(tumor_dataset, BATCH_SIZE, sampler = train_sampler)\n",
    "validationloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = validation_sampler)\n",
    "testloader = torch.utils.data.DataLoader(tumor_dataset, 1, sampler = test_sampler)\n",
    "\n",
    "print('Number of files in the train set: %s \\nNumber of files in the validation set: %s \\nNumber of files in the test set: %s' \\\n",
    "      % (len(train_indices), len(validation_indices), len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchio\n",
    "from torchio.transforms import Rescale, RandomAffine\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some visualization functions\n",
    "\n",
    "def show_nifti(image_path_or_image, colormap='gray'):\n",
    "    try:\n",
    "        from niwidgets import NiftiWidget\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "            widget = NiftiWidget(image_path_or_image)\n",
    "            widget.nifti_plotter(colormap=colormap)\n",
    "    except Exception:\n",
    "        if isinstance(image_path_or_image, nib.AnalyzeImage):\n",
    "            nii = image_path_or_image\n",
    "        else:\n",
    "            image_path = image_path_or_image\n",
    "            nii = nib.load(str(image_path))\n",
    "        k = int(nii.shape[-1] / 2)\n",
    "        plt.imshow(nii.dataobj[..., k], cmap=colormap)\n",
    "\n",
    "def show_sample(sample, image_name, label_name=None):\n",
    "    if label_name is not None:\n",
    "        sample = copy.deepcopy(sample)\n",
    "        affine = sample[label_name][AFFINE]\n",
    "        label = sample[label_name][DATA][0].numpy().astype(np.uint8)\n",
    "        label_image = torchio.utils.nib_to_sitk(label, affine)\n",
    "        border = sitk.BinaryContour(label_image)\n",
    "        border_array, _ = torchio.utils.sitk_to_nib(border)\n",
    "        border_tensor = torch.from_numpy(border_array)\n",
    "        image_tensor = sample[image_name][DATA][0]\n",
    "        image_tensor[border_tensor > 0.5] = image_tensor.max()\n",
    "    with tempfile.NamedTemporaryFile(suffix='.nii') as f:\n",
    "        torchio.ImagesDataset.save_sample(sample, {image_name: f.name})\n",
    "        show_nifti(f.name)\n",
    "\n",
    "def plot_histogram(axis, tensor, num_positions=100, label=None, alpha=0.05, color=None):\n",
    "    values = tensor.numpy().ravel()\n",
    "    kernel = stats.gaussian_kde(values)\n",
    "    positions = np.linspace(values.min(), values.max(), num=num_positions)\n",
    "    histogram = kernel(positions)\n",
    "    kwargs = dict(linewidth=1, color='black' if color is None else color, alpha=alpha)\n",
    "    if label is not None:\n",
    "        kwargs['label'] = label\n",
    "    axis.plot(positions, histogram, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 335 subjects\n"
     ]
    }
   ],
   "source": [
    "dataPath = Path('./data')\n",
    "\n",
    "t1_path = dataPath / 'all_gbm_pre_reg'\n",
    "tumor_path = dataPath / 'all_tumors_reg'\n",
    "\n",
    "t1_locations = sorted(t1_path.glob('*.nii.gz'))\n",
    "tumor_locations = sorted(tumor_path.glob('*.nii.gz'))\n",
    "\n",
    "\n",
    "assert len(t1_locations) == len(tumor_locations)\n",
    "\n",
    "subjects = []\n",
    "for (image_path, label_path) in zip(t1_locations, tumor_locations):\n",
    "    subject = torchio.Subject(\n",
    "        {'image': torchio.Image(image_path, torchio.INTENSITY),\n",
    "        'mask': torchio.Image(label_path, torchio.LABEL)}\n",
    "    )\n",
    "    subjects.append(subject)\n",
    "    \n",
    "dataset = torchio.ImagesDataset(subjects)\n",
    "print('Dataset size:', len(dataset), 'subjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define online augmentation function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an online augmentation function, but without affine augmentation\n",
    "\n",
    "def ol_aug_noAffine(data): \n",
    "    # data is a class 'dict' from trainlaoder\n",
    "    spatial_augmentation_type = np.random.choice(3,1, p = [0.1, 0.45, 0.45]).item()\n",
    "        # no transform, RandomNoise, RandomBiasField, RandomMotion, RandomGhosting\n",
    "        # For now only introduce random Gaussian noise\n",
    "    intensity_augmentation_type = np.random.choice(4,1, p = [0.05, 0.32, 0.32, 0.31]).item()\n",
    "\n",
    "#     if spatial_augmentation_type == 1:\n",
    "#         spatial_transform = RandomAffine(scales=(0.9,1.1),degrees=(-1,1),\n",
    "#                                         image_interpolation=Interpolation.NEAREST,seed=seed)\n",
    "    if spatial_augmentation_type == 1:\n",
    "        spatial_transform = RandomElasticDeformation(num_control_points=20,locked_borders=2,\n",
    "                                                     proportion_to_augment=1, \n",
    "                                                     image_interpolation=Interpolation.NEAREST,\n",
    "                                                     seed=seed) \n",
    "                                                     #deformation_std = 50)\n",
    "    elif spatial_augmentation_type == 2:\n",
    "        axis = np.random.choice([0,2],1).item()\n",
    "        spatial_transform = RandomFlip(seed = seed, flip_probability = 1, axes = axis)\n",
    "\n",
    "    if intensity_augmentation_type == 1: \n",
    "        intensity_transform = RandomNoise(seed = seed, std = (0, 5))\n",
    "    elif intensity_augmentation_type == 2: \n",
    "        intensity_transform = RandomBlur(std=(0, 0.5),seed=seed)\n",
    "    elif intensity_augmentation_type == 3:\n",
    "        intensity_transform = RandomGhosting(seed = seed, \n",
    "                                                 proportion_to_augment = 1, \n",
    "                                                 num_ghosts = (1, 3))\n",
    "\n",
    "    # Apply spatial transform. \n",
    "    # spatial_transformed_subject = spatial_transform(subject) if spatial_augmentation_type > 0 else subject\n",
    "    transformed_data = spatial_transform(data) if spatial_augmentation_type > 0 else data\n",
    "    transformed_data = intensity_transform(transformed_data) if intensity_augmentation_type > 0 else transformed_data\n",
    "\n",
    "    data = transformed_data\n",
    "    \n",
    "    return data\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training process\n",
    "### create a new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_classifier = None\n",
    "criterion = nn.BCELoss()\n",
    "learning_rate = 0.0001\n",
    "\n",
    "#### If you want to see the training trend within each epoch, you can change mini_batch to a positive integer \n",
    "#### that is no larger than the number of batches per epoch.\n",
    "mini_batch = False\n",
    "\n",
    "# Define where to save the model parameters.\n",
    "model_save_path = './online_aug_models/'\n",
    "os.makedirs(model_save_path, exist_ok = True)\n",
    "\n",
    "# New model is created.\n",
    "unet_model = U_Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Process\n",
      "Epoch:   1,  train Loss: 0.05466,  train score: 0.05925,  validation Loss: 0.05609,  validation score: 0.00013,  current lr:  0.0001 , Time: 918.71 s\tBest model saved at score: 0.00013\n",
      "Epoch:   2,  train Loss: 0.00907,  train score: 0.00295,  validation Loss: 0.04366,  validation score: 0.00002,  current lr:  0.0001 , Time: 916.29 s\n",
      "Epoch:   3,  train Loss: 0.00735,  train score: 0.00488,  validation Loss: 0.04046,  validation score: 0.00002,  current lr:  0.0001 , Time: 917.86 s\n",
      "Epoch:   4,  train Loss: 0.00714,  train score: 0.00547,  validation Loss: 0.04123,  validation score: 0.00002,  current lr:  0.0001 , Time: 913.63 s\n",
      "Epoch:   5,  train Loss: 0.00702,  train score: 0.00784,  validation Loss: 0.03986,  validation score: 0.00003,  current lr:  0.0001 , Time: 913.53 s\n",
      "Epoch:   6,  train Loss: 0.00671,  train score: 0.00308,  validation Loss: 0.03763,  validation score: 0.00007,  current lr:  0.0001 , Time: 913.89 s\n"
     ]
    }
   ],
   "source": [
    "# first load the model trained with offline augmented data\n",
    "# loading the previous best model (from online augmentation)\n",
    "# for the new online augmentation without affine and different probability\n",
    "\n",
    "# a new epoch\n",
    "epochs = 80\n",
    "\n",
    "\n",
    "# Training session history data.\n",
    "history = {'train_loss': list(), 'validation_loss': list()}\n",
    "\n",
    "# For save best feature. Initial loss taken a very high value.\n",
    "last_score = 0\n",
    "\n",
    "# Optimizer used for training process. Adam Optimizer.\n",
    "optimizer = optim.Adam(unet_model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Reducing LR on plateau feature to improve training.\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.85, patience = 2, verbose = True)\n",
    "\n",
    "print('Starting Training Process')\n",
    "\n",
    "assert validationloader.batch_size == 1\n",
    "    \n",
    "\n",
    "# Epoch Loop\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #################################### Train ####################################################\n",
    "    unet_model.train()\n",
    "    start_time = time()\n",
    "    # Training a single epoch\n",
    "    train_epoch_loss, train_batch_loss, batch_iteration = 0, 0, 0\n",
    "    train_score, validation_score, validation_loss = 0, 0, 0\n",
    "    \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "            \n",
    "    for batch, data in enumerate(trainloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Loading data to device used.\n",
    "        data = ol_aug_noAffine(data)\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "        \n",
    "        #image = image.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        #mask = mask.permute(0, 3, 1, 2).view([BATCH_SIZE,155,240,240])\n",
    "        # Clearing gradients of optimizer.\n",
    "        optimizer.zero_grad()\n",
    "        # Calculation predicted output using forward pass.\n",
    "        output = unet_model(image)\n",
    "        # Calculating the loss value.\n",
    "        loss_value = criterion(output, mask)\n",
    "        # Computing the gradients.\n",
    "        loss_value.backward()\n",
    "        # Optimizing the network parameters.\n",
    "        optimizer.step()\n",
    "        # Updating the running training loss\n",
    "        train_epoch_loss += loss_value.item()\n",
    "        train_batch_loss += loss_value.item()\n",
    "        \n",
    "        mask_prediction = unet_model(image)\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        mask = mask.cpu().numpy()\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        train_score += dice_coefficient(mask_prediction, mask)\n",
    "    \n",
    "\n",
    "        # Printing batch logs if any. Useful if you want to see the training trends within each epoch.\n",
    "        if mini_batch:\n",
    "            if (batch + 1) % mini_batch == 0:\n",
    "                train_batch_loss = train_batch_loss / (mini_batch * trainloader.batch_size)\n",
    "                print(\n",
    "                    f'    Batch: {batch + 1:2d},\\tBatch Loss: {train_batch_loss:.7f}')\n",
    "                train_batch_loss = 0\n",
    "    \n",
    "    \n",
    "    unet_train = train_score / batch_iteration\n",
    "    train_epoch_loss = train_epoch_loss / (batch_iteration * trainloader.batch_size)\n",
    "    \n",
    "    ################################### Validation ##################################################\n",
    "    unet_model.eval()\n",
    "    # To get data in loops.\n",
    "    batch_iteration = 0\n",
    "\n",
    "    for batch, data in enumerate(validationloader):\n",
    "        # Keeping track how many iteration is happening.\n",
    "        batch_iteration += 1\n",
    "        # Data prepared to be given as input to model.\n",
    "        image = data['image'].to(device, dtype=torch.float)\n",
    "        mask = data['mask'].to(device, dtype=torch.float)\n",
    "\n",
    "        # Predicted output from the input sample.\n",
    "        mask_prediction = unet_model(image)\n",
    "        \n",
    "        # comput validation loss\n",
    "        loss_value = criterion(mask_prediction, mask)\n",
    "        validation_loss += loss_value.item()\n",
    "        \n",
    "        # Threshold elimination.\n",
    "        mask_prediction = (mask_prediction > 0.5)\n",
    "        mask_prediction = mask_prediction.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        mask = np.resize(mask, (155, 240, 240))\n",
    "        mask_prediction = np.resize(mask_prediction, (155, 240, 240))\n",
    "        # Calculate the dice score for original and predicted image mask.\n",
    "        validation_score += dice_coefficient(mask_prediction, mask)\n",
    "\n",
    "    # Calculating the mean score for the whole validation dataset.\n",
    "    unet_val = validation_score / batch_iteration\n",
    "    validation_loss = validation_loss / batch_iteration\n",
    "    \n",
    "    # Collecting all epoch loss values for future visualization.\n",
    "    history['train_loss'].append(train_epoch_loss)\n",
    "    history['validation_loss'].append(validation_loss)\n",
    "    \n",
    "    # Reduce LR On Plateau\n",
    "    scheduler.step(validation_loss)\n",
    "\n",
    "    time_taken = time() - start_time\n",
    "    \n",
    "    # Training Logs printed.\n",
    "    print(f'Epoch: {epoch + 1:3d},  ', end = '')\n",
    "    print(f'train Loss: {train_epoch_loss:.5f},  ', end = '')\n",
    "    print(f'train score: {unet_train:.5f},  ', end = '')\n",
    "    print(f'validation Loss: {validation_loss:.5f},  ', end = '')\n",
    "    print(f'validation score: {unet_val:.5f},  ', end = '')\n",
    "\n",
    "    for pg in optimizer.param_groups:\n",
    "        print('current lr: ', pg['lr'], ', ', end = '')\n",
    "    print(f'Time: {time_taken:.2f} s', end = '')\n",
    "\n",
    "    # Save the model every epoch.\n",
    "    #current_epoch_model_save_path = os.path.join(model_save_path, 'Basic_Unet_epoch_%s.pth' % (str(epoch).zfill(3)))\n",
    "    #torch.save(unet_model.state_dict(), current_epoch_model_save_path)\n",
    "    \n",
    "    # Save the best model (determined by validation score) and give it a unique name.\n",
    "    best_model_path = os.path.join(model_save_path, 'new_ori_torchio.pth')\n",
    "    if  last_score < unet_val:\n",
    "        torch.save(unet_model.state_dict(), best_model_path)\n",
    "        last_score = unet_val\n",
    "        print(f'\\tBest model saved at score: {unet_val:.5f}')\n",
    "    else:\n",
    "        print()\n",
    "\n",
    "print(f'Training Finished after {epochs} epoches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 BMEN4460",
   "language": "python",
   "name": "bmen4460"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
